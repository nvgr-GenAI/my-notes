"use strict";(self.webpackChunkmy_notes=self.webpackChunkmy_notes||[]).push([[2673],{6766:(e,s,i)=>{i.r(s),i.d(s,{assets:()=>l,contentTitle:()=>o,default:()=>m,frontMatter:()=>r,metadata:()=>n,toc:()=>c});const n=JSON.parse('{"id":"ml/supervised/random-forest-classification","title":"Random Forest Classification","description":"Random Forest Classification is an ensemble learning method that builds multiple decision trees and merges their predictions to achieve more accurate and stable results. It\'s particularly effective for classification tasks.","source":"@site/docs/ml/supervised/random-forest-classification.md","sourceDirName":"ml/supervised","slug":"/ml/supervised/random-forest-classification","permalink":"/my-notes/docs/ml/supervised/random-forest-classification","draft":false,"unlisted":false,"editUrl":"https://github.com/nvgr-GenAI/my-notes/edit/main/docs/ml/supervised/random-forest-classification.md","tags":[],"version":"current","frontMatter":{},"sidebar":"mlSidebar","previous":{"title":"Decision Tree Classification","permalink":"/my-notes/docs/ml/supervised/decision-tree-classification"},"next":{"title":"Naive Bayes","permalink":"/my-notes/docs/ml/supervised/naive-bayes"}}');var t=i(4848),a=i(8453);const r={},o="Random Forest Classification",l={},c=[{value:"How Random Forest Classification Works",id:"how-random-forest-classification-works",level:2},{value:"Advantages",id:"advantages",level:2},{value:"Disadvantages",id:"disadvantages",level:2},{value:"Implementation",id:"implementation",level:2},{value:"Hyperparameters",id:"hyperparameters",level:2},{value:"Applications",id:"applications",level:2}];function d(e){const s={code:"code",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(s.header,{children:(0,t.jsx)(s.h1,{id:"random-forest-classification",children:"Random Forest Classification"})}),"\n",(0,t.jsxs)(s.p,{children:["Random Forest Classification is an ",(0,t.jsx)(s.strong,{children:"ensemble learning method"})," that builds multiple decision trees and merges their predictions to achieve more accurate and stable results. It's particularly effective for classification tasks."]}),"\n",(0,t.jsx)(s.h2,{id:"how-random-forest-classification-works",children:"How Random Forest Classification Works"}),"\n",(0,t.jsxs)(s.ol,{children:["\n",(0,t.jsxs)(s.li,{children:[(0,t.jsx)(s.strong,{children:"Bootstrap Aggregating (Bagging)"}),": Creates multiple subsets of the training data with replacement."]}),"\n",(0,t.jsxs)(s.li,{children:[(0,t.jsx)(s.strong,{children:"Decision Tree Creation"}),": Builds a decision tree for each subset, but with a random subset of features at each split."]}),"\n",(0,t.jsxs)(s.li,{children:[(0,t.jsx)(s.strong,{children:"Voting"}),": For classification, the final prediction is determined by majority vote across all trees."]}),"\n"]}),"\n",(0,t.jsx)(s.h2,{id:"advantages",children:"Advantages"}),"\n",(0,t.jsxs)(s.ul,{children:["\n",(0,t.jsx)(s.li,{children:"Robust against overfitting compared to single decision trees"}),"\n",(0,t.jsx)(s.li,{children:"Handles high-dimensional data well without feature selection"}),"\n",(0,t.jsx)(s.li,{children:"Provides feature importance measures"}),"\n",(0,t.jsx)(s.li,{children:"Effective for imbalanced datasets"}),"\n",(0,t.jsx)(s.li,{children:"Can handle missing values and maintain good accuracy"}),"\n"]}),"\n",(0,t.jsx)(s.h2,{id:"disadvantages",children:"Disadvantages"}),"\n",(0,t.jsxs)(s.ul,{children:["\n",(0,t.jsx)(s.li,{children:"Less interpretable than a single decision tree"}),"\n",(0,t.jsx)(s.li,{children:"Computationally more intensive than simpler algorithms"}),"\n",(0,t.jsx)(s.li,{children:"Can be overfit with noisy datasets"}),"\n",(0,t.jsx)(s.li,{children:"May be biased towards features with more levels in categorical variables"}),"\n"]}),"\n",(0,t.jsx)(s.h2,{id:"implementation",children:"Implementation"}),"\n",(0,t.jsx)(s.pre,{children:(0,t.jsx)(s.code,{className:"language-python",children:"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Generate a sample dataset\nX, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Create and train the model\nrf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\nrf_classifier.fit(X_train, y_train)\n\n# Make predictions\ny_pred = rf_classifier.predict(X_test)\n\n# Evaluate the model\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred))\n\n# Feature importance\nfeature_importance = pd.DataFrame({\n    'Feature': range(X.shape[1]),\n    'Importance': rf_classifier.feature_importances_\n}).sort_values('Importance', ascending=False)\nprint(\"\\nFeature Importance:\")\nprint(feature_importance.head(10))\n"})}),"\n",(0,t.jsx)(s.h2,{id:"hyperparameters",children:"Hyperparameters"}),"\n",(0,t.jsxs)(s.ul,{children:["\n",(0,t.jsxs)(s.li,{children:[(0,t.jsx)(s.code,{children:"n_estimators"}),": Number of trees in the forest"]}),"\n",(0,t.jsxs)(s.li,{children:[(0,t.jsx)(s.code,{children:"max_depth"}),": Maximum depth of each tree"]}),"\n",(0,t.jsxs)(s.li,{children:[(0,t.jsx)(s.code,{children:"min_samples_split"}),": Minimum samples required to split a node"]}),"\n",(0,t.jsxs)(s.li,{children:[(0,t.jsx)(s.code,{children:"min_samples_leaf"}),": Minimum samples required in a leaf node"]}),"\n",(0,t.jsxs)(s.li,{children:[(0,t.jsx)(s.code,{children:"max_features"}),": Number of features to consider for the best split"]}),"\n"]}),"\n",(0,t.jsx)(s.h2,{id:"applications",children:"Applications"}),"\n",(0,t.jsxs)(s.ul,{children:["\n",(0,t.jsx)(s.li,{children:"Customer churn prediction"}),"\n",(0,t.jsx)(s.li,{children:"Credit card fraud detection"}),"\n",(0,t.jsx)(s.li,{children:"Disease diagnosis"}),"\n",(0,t.jsx)(s.li,{children:"Image classification"}),"\n",(0,t.jsx)(s.li,{children:"Sentiment analysis"}),"\n"]})]})}function m(e={}){const{wrapper:s}={...(0,a.R)(),...e.components};return s?(0,t.jsx)(s,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,s,i)=>{i.d(s,{R:()=>r,x:()=>o});var n=i(6540);const t={},a=n.createContext(t);function r(e){const s=n.useContext(a);return n.useMemo((function(){return"function"==typeof e?e(s):{...s,...e}}),[s,e])}function o(e){let s;return s=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),n.createElement(a.Provider,{value:s},e.children)}}}]);