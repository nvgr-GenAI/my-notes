"use strict";(self.webpackChunkmy_notes=self.webpackChunkmy_notes||[]).push([[7022],{5057:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>o,contentTitle:()=>a,default:()=>h,frontMatter:()=>l,metadata:()=>r,toc:()=>d});const r=JSON.parse('{"id":"ml/supervised/ridge-regression","title":"Ridge Regression","description":"Ridge Regression is a regularized version of Linear Regression that adds an L2 penalty term to the cost function. This technique helps prevent overfitting and handles multicollinearity by shrinking the coefficient estimates toward zero.","source":"@site/docs/ml/supervised/ridge-regression.md","sourceDirName":"ml/supervised","slug":"/ml/supervised/ridge-regression","permalink":"/my-notes/docs/ml/supervised/ridge-regression","draft":false,"unlisted":false,"editUrl":"https://github.com/nvgr-GenAI/my-notes/edit/main/docs/ml/supervised/ridge-regression.md","tags":[],"version":"current","frontMatter":{},"sidebar":"mlSidebar","previous":{"title":"Polynomial Regression","permalink":"/my-notes/docs/ml/supervised/polynomial-regression"},"next":{"title":"Lasso Regression","permalink":"/my-notes/docs/ml/supervised/lasso-regression"}}');var s=i(4848),t=i(8453);const l={},a="Ridge Regression",o={},d=[{value:"How Ridge Regression Works",id:"how-ridge-regression-works",level:2},{value:"Advantages",id:"advantages",level:2},{value:"Disadvantages",id:"disadvantages",level:2},{value:"Implementation",id:"implementation",level:2},{value:"Selecting the Regularization Parameter (\u03b1)",id:"selecting-the-regularization-parameter-\u03b1",level:2},{value:"Applications",id:"applications",level:2}];function c(e){const n={code:"code",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"ridge-regression",children:"Ridge Regression"})}),"\n",(0,s.jsxs)(n.p,{children:["Ridge Regression is a regularized version of ",(0,s.jsx)(n.strong,{children:"Linear Regression"})," that adds an L2 penalty term to the cost function. This technique helps prevent overfitting and handles multicollinearity by shrinking the coefficient estimates toward zero."]}),"\n",(0,s.jsx)(n.h2,{id:"how-ridge-regression-works",children:"How Ridge Regression Works"}),"\n",(0,s.jsx)(n.p,{children:"Ridge Regression minimizes the following objective function:"}),"\n",(0,s.jsx)("div",{class:"math",children:(0,s.jsxs)(n.p,{children:["min",(0,s.jsx)("sub",{children:"\u03b2"})," \u03a3",(0,s.jsx)("sub",{children:"i=1"}),(0,s.jsx)("sup",{children:"n"}),"(y",(0,s.jsx)("sub",{children:"i"})," - \u03b2",(0,s.jsx)("sub",{children:"0"})," - \u03a3",(0,s.jsx)("sub",{children:"j=1"}),(0,s.jsx)("sup",{children:"p"}),"\u03b2",(0,s.jsx)("sub",{children:"j"}),"x",(0,s.jsx)("sub",{children:"ij"}),")",(0,s.jsx)("sup",{children:"2"})," + \u03bb\u03a3",(0,s.jsx)("sub",{children:"j=1"}),(0,s.jsx)("sup",{children:"p"}),"\u03b2",(0,s.jsx)("sub",{children:"j"}),(0,s.jsx)("sup",{children:"2"})]})}),"\n",(0,s.jsx)(n.p,{children:"Where:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["y",(0,s.jsx)("sub",{children:"i"})," is the target variable"]}),"\n",(0,s.jsxs)(n.li,{children:["x",(0,s.jsx)("sub",{children:"ij"})," are the features"]}),"\n",(0,s.jsxs)(n.li,{children:["\u03b2",(0,s.jsx)("sub",{children:"0"}),", \u03b2",(0,s.jsx)("sub",{children:"j"})," are the coefficients"]}),"\n",(0,s.jsx)(n.li,{children:"\u03bb is the regularization parameter"}),"\n",(0,s.jsxs)(n.li,{children:["The term \u03bb\u03a3",(0,s.jsx)("sub",{children:"j=1"}),(0,s.jsx)("sup",{children:"p"}),"\u03b2",(0,s.jsx)("sub",{children:"j"}),(0,s.jsx)("sup",{children:"2"})," is the L2 regularization penalty"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"advantages",children:"Advantages"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Reduces model complexity and prevents overfitting"}),"\n",(0,s.jsx)(n.li,{children:"Effective for handling multicollinearity"}),"\n",(0,s.jsx)(n.li,{children:"Can improve model generalization"}),"\n",(0,s.jsx)(n.li,{children:"Coefficients shrink toward zero but rarely become exactly zero"}),"\n",(0,s.jsx)(n.li,{children:"Stable when features are highly correlated"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"disadvantages",children:"Disadvantages"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Not suitable for feature selection (all features remain in the model)"}),"\n",(0,s.jsx)(n.li,{children:"Less interpretable than simple linear regression"}),"\n",(0,s.jsx)(n.li,{children:"Requires tuning of the regularization parameter"}),"\n",(0,s.jsx)(n.li,{children:"May underfit with very high regularization"}),"\n",(0,s.jsx)(n.li,{children:"Performance depends heavily on feature scaling"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"implementation",children:"Implementation"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"from sklearn.linear_model import Ridge\nfrom sklearn.datasets import make_regression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate sample data\nX, y = make_regression(n_samples=200, n_features=50, noise=0.5, random_state=42)\n\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Scale the features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Create and train the Ridge model\nalphas = [0.001, 0.01, 0.1, 1, 10, 100]\nplt.figure(figsize=(12, 8))\n\nfor i, alpha in enumerate(alphas):\n    ridge_model = Ridge(alpha=alpha)\n    ridge_model.fit(X_train_scaled, y_train)\n    \n    # Make predictions\n    y_pred = ridge_model.predict(X_test_scaled)\n    \n    # Evaluate the model\n    mse = mean_squared_error(y_test, y_pred)\n    r2 = r2_score(y_test, y_pred)\n    \n    print(f\"Alpha: {alpha}, MSE: {mse:.4f}, R\xb2: {r2:.4f}\")\n    \n    # Plot coefficients\n    plt.subplot(2, 3, i+1)\n    plt.stem(range(X_train.shape[1]), ridge_model.coef_)\n    plt.xlabel('Feature')\n    plt.ylabel('Coefficient Value')\n    plt.title(f'Alpha = {alpha}\\nMSE = {mse:.4f}, R\xb2 = {r2:.4f}')\n\nplt.tight_layout()\nplt.show()\n\n# Cross-validation to find optimal alpha\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.model_selection import KFold\n\n# Define range of alphas and cross-validation strategy\nalphas = np.logspace(-3, 3, 50)\ncv = KFold(n_splits=5, shuffle=True, random_state=42)\n\n# Create and fit RidgeCV model\nridge_cv = RidgeCV(alphas=alphas, cv=cv, scoring='neg_mean_squared_error')\nridge_cv.fit(X_train_scaled, y_train)\n\n# Print optimal alpha\nprint(f\"Optimal Alpha: {ridge_cv.alpha_}\")\n\n# Train final model with optimal alpha\noptimal_ridge = Ridge(alpha=ridge_cv.alpha_)\noptimal_ridge.fit(X_train_scaled, y_train)\ny_pred = optimal_ridge.predict(X_test_scaled)\n\n# Evaluate final model\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(f\"Final Model - MSE: {mse:.4f}, R\xb2: {r2:.4f}\")\n"})}),"\n",(0,s.jsx)(n.h2,{id:"selecting-the-regularization-parameter-\u03b1",children:"Selecting the Regularization Parameter (\u03b1)"}),"\n",(0,s.jsx)(n.p,{children:"The regularization parameter \u03b1 controls the strength of the regularization:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Small \u03b1"}),": Close to ordinary least squares, might not solve multicollinearity issues"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Large \u03b1"}),": More regularization, coefficients approach zero, might underfit"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Common methods to select optimal \u03b1 include:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Cross-validation"}),"\n",(0,s.jsx)(n.li,{children:"Information criteria (AIC, BIC)"}),"\n",(0,s.jsx)(n.li,{children:"Domain knowledge"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"applications",children:"Applications"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Financial market prediction"}),"\n",(0,s.jsx)(n.li,{children:"Housing price prediction"}),"\n",(0,s.jsx)(n.li,{children:"Genomic data analysis"}),"\n",(0,s.jsx)(n.li,{children:"Research with many correlated predictors"}),"\n",(0,s.jsx)(n.li,{children:"Any regression context where features exhibit multicollinearity"}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>l,x:()=>a});var r=i(6540);const s={},t=r.createContext(s);function l(e){const n=r.useContext(t);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:l(e.components),r.createElement(t.Provider,{value:n},e.children)}}}]);