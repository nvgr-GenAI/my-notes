"use strict";(self.webpackChunkmy_notes=self.webpackChunkmy_notes||[]).push([[193],{3302:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>p,frontMatter:()=>a,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"ml/supervised/decision-tree-classification","title":"Decision Tree Classification","description":"Understanding Decision Tree algorithms for classification tasks","source":"@site/docs/ml/supervised/decision-tree-classification.md","sourceDirName":"ml/supervised","slug":"/ml/supervised/decision-tree-classification","permalink":"/my-notes/docs/ml/supervised/decision-tree-classification","draft":false,"unlisted":false,"editUrl":"https://github.com/nvgr-GenAI/my-notes/edit/main/docs/ml/supervised/decision-tree-classification.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"title":"Decision Tree Classification","sidebar_position":3,"description":"Understanding Decision Tree algorithms for classification tasks"},"sidebar":"mlSidebar","previous":{"title":"Logistic Regression","permalink":"/my-notes/docs/ml/supervised/logistic-regression"},"next":{"title":"Random Forest Classification","permalink":"/my-notes/docs/ml/supervised/random-forest-classification"}}');var r=i(4848),s=i(8453);const a={title:"Decision Tree Classification",sidebar_position:3,description:"Understanding Decision Tree algorithms for classification tasks"},o="Decision Tree Classification",l={},d=[{value:"1. How Classification Trees Work",id:"1-how-classification-trees-work",level:2},{value:"Splitting Criteria:",id:"splitting-criteria",level:3},{value:"Stopping Criteria:",id:"stopping-criteria",level:3},{value:"2. Example Use Case: Loan Approval Prediction",id:"2-example-use-case-loan-approval-prediction",level:2},{value:"Scenario",id:"scenario",level:3},{value:"Dataset Sample",id:"dataset-sample",level:3},{value:"Decision Tree Approach",id:"decision-tree-approach",level:3},{value:"3. Advantages of Decision Trees for Classification",id:"3-advantages-of-decision-trees-for-classification",level:2},{value:"4. Limitations",id:"4-limitations",level:2},{value:"5. Real-World Applications",id:"5-real-world-applications",level:2},{value:"6. Implementation Example",id:"6-implementation-example",level:2},{value:"Summary",id:"summary",level:2}];function c(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"decision-tree-classification",children:"Decision Tree Classification"})}),"\n",(0,r.jsxs)(n.p,{children:["Decision Trees for classification are ",(0,r.jsx)(n.strong,{children:"supervised learning algorithms"})," used to predict categorical target variables. They work by creating a model that predicts the class of a target variable by learning simple decision rules inferred from the data features."]}),"\n",(0,r.jsx)(n.h2,{id:"1-how-classification-trees-work",children:"1. How Classification Trees Work"}),"\n",(0,r.jsxs)(n.p,{children:["Decision trees use a ",(0,r.jsx)(n.strong,{children:"hierarchical, tree-like structure"})," where:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Root Node"}),": Starting point, represents entire dataset"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Decision Nodes"}),": Points where data is split based on feature values"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Leaf Nodes"}),": Terminal nodes that provide the classification output"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"The algorithm works by:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Selecting the best feature"})," to split the data at each node"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Creating child nodes"})," based on the split"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Repeating recursively"})," until stopping criteria are met"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"splitting-criteria",children:"Splitting Criteria:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Gini Impurity"}),": Measures the probability of incorrect classification"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Entropy"}),": Measures the level of disorder or uncertainty"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Information Gain"}),": The decrease in entropy after a split"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"stopping-criteria",children:"Stopping Criteria:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Maximum depth reached"}),"\n",(0,r.jsx)(n.li,{children:"Minimum samples required for a split"}),"\n",(0,r.jsx)(n.li,{children:"Minimum samples required at a leaf node"}),"\n",(0,r.jsx)(n.li,{children:"Maximum leaf nodes reached"}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"2-example-use-case-loan-approval-prediction",children:"2. Example Use Case: Loan Approval Prediction"}),"\n",(0,r.jsx)(n.h3,{id:"scenario",children:"Scenario"}),"\n",(0,r.jsx)(n.p,{children:"A bank wants to predict whether to approve loan applications based on applicant characteristics."}),"\n",(0,r.jsx)(n.h3,{id:"dataset-sample",children:"Dataset Sample"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Age"}),(0,r.jsx)(n.th,{children:"Income"}),(0,r.jsx)(n.th,{children:"Owns House"}),(0,r.jsx)(n.th,{children:"Credit Score"}),(0,r.jsx)(n.th,{children:"Loan Approved"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"25"}),(0,r.jsx)(n.td,{children:"$45K"}),(0,r.jsx)(n.td,{children:"No"}),(0,r.jsx)(n.td,{children:"Good"}),(0,r.jsx)(n.td,{children:"No"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"42"}),(0,r.jsx)(n.td,{children:"$120K"}),(0,r.jsx)(n.td,{children:"Yes"}),(0,r.jsx)(n.td,{children:"Excellent"}),(0,r.jsx)(n.td,{children:"Yes"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"30"}),(0,r.jsx)(n.td,{children:"$70K"}),(0,r.jsx)(n.td,{children:"No"}),(0,r.jsx)(n.td,{children:"Good"}),(0,r.jsx)(n.td,{children:"Yes"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"35"}),(0,r.jsx)(n.td,{children:"$50K"}),(0,r.jsx)(n.td,{children:"No"}),(0,r.jsx)(n.td,{children:"Poor"}),(0,r.jsx)(n.td,{children:"No"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"60"}),(0,r.jsx)(n.td,{children:"$80K"}),(0,r.jsx)(n.td,{children:"Yes"}),(0,r.jsx)(n.td,{children:"Fair"}),(0,r.jsx)(n.td,{children:"No"})]})]})]}),"\n",(0,r.jsx)(n.h3,{id:"decision-tree-approach",children:"Decision Tree Approach"}),"\n",(0,r.jsx)(n.p,{children:"The algorithm will:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Select the most informative feature for the first split (e.g., Credit Score)"}),"\n",(0,r.jsx)(n.li,{children:"Split the data based on this feature"}),"\n",(0,r.jsx)(n.li,{children:"Continue splitting recursively on each subset"}),"\n",(0,r.jsx)(n.li,{children:"Form a tree structure that can predict loan approval for new applicants"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"For example, the resulting tree might look like:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Credit Score?\n\u251c\u2500\u2500 Poor \u2192 Deny Loan\n\u251c\u2500\u2500 Fair \n\u2502   \u251c\u2500\u2500 Income > $100K? \u2192 Approve Loan\n\u2502   \u2514\u2500\u2500 Income \u2264 $100K? \u2192 Deny Loan\n\u251c\u2500\u2500 Good\n\u2502   \u251c\u2500\u2500 Owns House? \u2192 Approve Loan\n\u2502   \u2514\u2500\u2500 Renting\n\u2502       \u251c\u2500\u2500 Income > $60K? \u2192 Approve Loan\n\u2502       \u2514\u2500\u2500 Income \u2264 $60K? \u2192 Deny Loan\n\u2514\u2500\u2500 Excellent \u2192 Approve Loan\n"})}),"\n",(0,r.jsx)(n.p,{children:"For a new applicant with:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Credit Score: Good"}),"\n",(0,r.jsx)(n.li,{children:"Owns House: No"}),"\n",(0,r.jsx)(n.li,{children:"Income: $75K"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"We would trace the path: Good \u2192 Renting \u2192 Income > $60K \u2192 Approve Loan"}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"3-advantages-of-decision-trees-for-classification",children:"3. Advantages of Decision Trees for Classification"}),"\n",(0,r.jsx)(n.p,{children:"\u2705 Simple to understand and visualize class boundaries"}),"\n",(0,r.jsx)(n.p,{children:"\u2705 Requires little data preprocessing (no normalization needed)"}),"\n",(0,r.jsx)(n.p,{children:"\u2705 Can handle both numerical and categorical data"}),"\n",(0,r.jsx)(n.p,{children:"\u2705 Implicitly performs feature selection"}),"\n",(0,r.jsx)(n.p,{children:"\u2705 Non-parametric (no assumptions about data distribution)"}),"\n",(0,r.jsx)(n.p,{children:"\u2705 Can handle multi-class problems naturally"}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"4-limitations",children:"4. Limitations"}),"\n",(0,r.jsx)(n.p,{children:"\u274c Prone to overfitting, especially with deep trees"}),"\n",(0,r.jsx)(n.p,{children:"\u274c Can be unstable (small variations in data can produce very different trees)"}),"\n",(0,r.jsx)(n.p,{children:"\u274c May create biased trees if classes are imbalanced"}),"\n",(0,r.jsx)(n.p,{children:"\u274c Greedy algorithms may not find the globally optimal tree"}),"\n",(0,r.jsxs)(n.p,{children:["Techniques like ",(0,r.jsx)(n.strong,{children:"pruning"}),", ",(0,r.jsx)(n.strong,{children:"setting maximum depth"}),", and ",(0,r.jsx)(n.strong,{children:"ensemble methods"})," (Random Forests) can address some of these limitations."]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"5-real-world-applications",children:"5. Real-World Applications"}),"\n",(0,r.jsxs)(n.p,{children:["\ud83c\udfe5 ",(0,r.jsx)(n.strong,{children:"Medical Diagnosis"})," \u2013 Predicting diseases based on symptoms"]}),"\n",(0,r.jsxs)(n.p,{children:["\ud83d\udcca ",(0,r.jsx)(n.strong,{children:"Customer Segmentation"})," \u2013 Grouping customers based on behavior"]}),"\n",(0,r.jsxs)(n.p,{children:["\ud83d\udcb3 ",(0,r.jsx)(n.strong,{children:"Credit Risk Assessment"})," \u2013 Determining credit approval"]}),"\n",(0,r.jsxs)(n.p,{children:["\ud83e\udd16 ",(0,r.jsx)(n.strong,{children:"Sentiment Analysis"})," \u2013 Classifying text sentiment"]}),"\n",(0,r.jsxs)(n.p,{children:["\ud83c\udfaf ",(0,r.jsx)(n.strong,{children:"Target Marketing"})," \u2013 Identifying likely customers for campaigns"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"6-implementation-example",children:"6. Implementation Example"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# Install Dependencies\n# pip install numpy pandas scikit-learn matplotlib seaborn graphviz pydotplus\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree, export_graphviz\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn.preprocessing import LabelEncoder\nimport graphviz\n\n# Sample loan approval dataset\ndata = {\n    'Age': [25, 42, 30, 35, 60, 48, 33, 27, 45, 52, 38, 29, 36, 55, 22, 43, 50, 31, 28, 40],\n    'Income': [45000, 120000, 70000, 50000, 80000, 90000, 65000, 35000, 110000, 75000, \n               60000, 40000, 85000, 95000, 30000, 100000, 130000, 55000, 47000, 72000],\n    'OwnsHouse': ['No', 'Yes', 'No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', \n                  'No', 'No', 'Yes', 'Yes', 'No', 'Yes', 'Yes', 'No', 'No', 'Yes'],\n    'CreditScore': ['Good', 'Excellent', 'Good', 'Poor', 'Fair', 'Good', 'Excellent', 'Fair', 'Good', 'Excellent',\n                    'Poor', 'Fair', 'Good', 'Fair', 'Poor', 'Excellent', 'Good', 'Fair', 'Good', 'Good'],\n    'LoanApproved': ['No', 'Yes', 'Yes', 'No', 'No', 'Yes', 'Yes', 'No', 'Yes', 'Yes',\n                     'No', 'No', 'Yes', 'No', 'No', 'Yes', 'Yes', 'No', 'No', 'Yes']\n}\n\n# Convert to DataFrame\ndf = pd.DataFrame(data)\n\n# Display the first few rows\nprint(df.head())\n\n# Convert categorical to numerical values\nle_owns_house = LabelEncoder()\nle_credit_score = LabelEncoder()\nle_loan_approved = LabelEncoder()\n\ndf['OwnsHouse_Encoded'] = le_owns_house.fit_transform(df['OwnsHouse'])\ndf['CreditScore_Encoded'] = le_credit_score.fit_transform(df['CreditScore'])\ndf['LoanApproved_Encoded'] = le_loan_approved.fit_transform(df['LoanApproved'])\n\n# Map the encoded values back to their original categories for better interpretation\ncredit_mapping = dict(zip(le_credit_score.transform(le_credit_score.classes_), le_credit_score.classes_))\nprint(\"\\nCredit Score Mapping:\", credit_mapping)\n\n# Basic data exploration\nprint(\"\\nData Summary:\")\nprint(df.describe())\n\nprint(\"\\nLoan Approval Distribution:\")\nprint(df['LoanApproved'].value_counts())\n\n# Visualize correlations\nplt.figure(figsize=(10, 8))\n# Select only numerical columns\nnumeric_df = df[['Age', 'Income', 'OwnsHouse_Encoded', 'CreditScore_Encoded', 'LoanApproved_Encoded']]\ncorrelation_matrix = numeric_df.corr()\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\nplt.title('Correlation Matrix')\nplt.show()\n\n# Visualize the relationship between features and loan approval\nplt.figure(figsize=(14, 10))\n\nplt.subplot(2, 2, 1)\nsns.boxplot(x='LoanApproved', y='Age', data=df)\nplt.title('Age vs. Loan Approval')\n\nplt.subplot(2, 2, 2)\nsns.boxplot(x='LoanApproved', y='Income', data=df)\nplt.title('Income vs. Loan Approval')\n\nplt.subplot(2, 2, 3)\nsns.countplot(x='OwnsHouse', hue='LoanApproved', data=df)\nplt.title('Home Ownership vs. Loan Approval')\n\nplt.subplot(2, 2, 4)\nsns.countplot(x='CreditScore', hue='LoanApproved', data=df)\nplt.title('Credit Score vs. Loan Approval')\n\nplt.tight_layout()\nplt.show()\n\n# Prepare data for the model\nX = df[['Age', 'Income', 'OwnsHouse_Encoded', 'CreditScore_Encoded']]\ny = df['LoanApproved_Encoded']\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n\n# Create a Decision Tree model\ndt_classifier = DecisionTreeClassifier(random_state=42)\n\n# Cross-validation to find optimal max_depth\ndepth_range = range(1, 10)\naccuracy_scores = []\n\nfor depth in depth_range:\n    dt_classifier = DecisionTreeClassifier(max_depth=depth, random_state=42)\n    scores = cross_val_score(dt_classifier, X_train, y_train, cv=5, scoring='accuracy')\n    accuracy_scores.append(scores.mean())\n\n# Plot cross-validation results\nplt.figure(figsize=(10, 6))\nplt.plot(depth_range, accuracy_scores, marker='o')\nplt.xlabel('Maximum Depth')\nplt.ylabel('Cross-Validation Accuracy')\nplt.title('Finding Optimal Tree Depth')\nplt.grid(True)\nplt.show()\n\n# Get the best depth\nbest_depth = depth_range[np.argmax(accuracy_scores)]\nprint(f\"\\nOptimal tree depth: {best_depth}\")\n\n# Train the final model with the best depth\ndt_classifier = DecisionTreeClassifier(max_depth=best_depth, random_state=42)\ndt_classifier.fit(X_train, y_train)\n\n# Make predictions\ny_pred = dt_classifier.predict(X_test)\n\n# Model evaluation\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy:.4f}\")\n\nprint(\"\\nClassification Report:\")\ntarget_names = ['Denied', 'Approved']\nprint(classification_report(y_test, y_pred, target_names=target_names))\n\nprint(\"\\nConfusion Matrix:\")\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=target_names, yticklabels=target_names)\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.show()\n\n# Visualize the decision tree\nplt.figure(figsize=(20, 10))\nfeature_names = ['Age', 'Income', 'Owns House', 'Credit Score']\nclass_names = ['Denied', 'Approved']\nplot_tree(dt_classifier, filled=True, feature_names=feature_names, class_names=class_names, rounded=True)\nplt.title(f\"Decision Tree (Max Depth = {best_depth})\")\nplt.show()\n\n# For a more detailed visualization using graphviz\ndot_data = export_graphviz(dt_classifier, out_file=None, \n                           feature_names=feature_names,\n                           class_names=class_names,\n                           filled=True, rounded=True,  \n                           special_characters=True)\ngraph = graphviz.Source(dot_data)\n# graph.render(\"loan_decision_tree\") # Uncomment to save the visualization\n\n# Feature importance\nimportances = dt_classifier.feature_importances_\nindices = np.argsort(importances)[::-1]\n\nplt.figure(figsize=(10, 6))\nplt.title('Feature Importances')\nplt.bar(range(X.shape[1]), importances[indices], align='center')\nplt.xticks(range(X.shape[1]), np.array(feature_names)[indices])\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nFeature Importances:\")\nfor i, feature in enumerate(np.array(feature_names)[indices]):\n    print(f\"{feature}: {importances[indices[i]]:.4f}\")\n\n# Predict on new data\nnew_applicant = np.array([[30, 75000, 0, le_credit_score.transform(['Good'])[0]]])\nprediction = dt_classifier.predict(new_applicant)\nprobability = dt_classifier.predict_proba(new_applicant)\n\nprint(\"\\nNew Applicant:\")\nprint(f\"Age: 30\")\nprint(f\"Income: $75,000\")\nprint(f\"Owns House: No\")\nprint(f\"Credit Score: Good\")\nprint(f\"\\nLoan Decision: {'Approved' if prediction[0] == 1 else 'Denied'}\")\nprint(f\"Probability: Denied: {probability[0][0]:.4f}, Approved: {probability[0][1]:.4f}\")\n\n# Find the path through the decision tree for the new applicant\ndef get_decision_path(tree, X, feature_names):\n    node_indicator = tree.decision_path(X)\n    leaf_id = tree.apply(X)\n    \n    feature = tree.tree_.feature\n    threshold = tree.tree_.threshold\n    \n    node_index = node_indicator.indices[node_indicator.indptr[0]:node_indicator.indptr[1]]\n    \n    rules = []\n    for node_id in node_index:\n        # Continue to the next node if not a leaf\n        if leaf_id[0] != node_id:\n            # If the feature value is below the threshold, follow the left path\n            if X[0, feature[node_id]] <= threshold[node_id]:\n                threshold_sign = \"<=\"\n            else:\n                threshold_sign = \">\"\n                \n            # Format the rule for better readability\n            if feature[node_id] == 2:  # OwnsHouse\n                value = \"Yes\" if threshold[node_id] < 0.5 else \"No\"\n                rules.append(f\"{feature_names[feature[node_id]]} = {value}\")\n            elif feature[node_id] == 3:  # CreditScore\n                value = list(credit_mapping.values())[int(threshold[node_id])]\n                rules.append(f\"{feature_names[feature[node_id]]} = {value}\")\n            else:\n                rules.append(f\"{feature_names[feature[node_id]]} {threshold_sign} {threshold[node_id]:.2f}\")\n    \n    return rules\n\ndecision_path = get_decision_path(dt_classifier, new_applicant, feature_names)\nprint(\"\\nDecision Path:\")\nfor i, rule in enumerate(decision_path):\n    print(f\"{i+1}. {rule}\")\n"})}),"\n",(0,r.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,r.jsx)(n.p,{children:"\u2705 Built a Decision Tree classification model for loan approval prediction"}),"\n",(0,r.jsx)(n.p,{children:"\u2705 Used cross-validation to find the optimal tree depth"}),"\n",(0,r.jsx)(n.p,{children:"\u2705 Visualized the decision tree for interpretability"}),"\n",(0,r.jsx)(n.p,{children:"\u2705 Analyzed feature importance to understand key factors"}),"\n",(0,r.jsx)(n.p,{children:"\u2705 Demonstrated making and explaining predictions for new applicants with classification probabilities"})]})}function p(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>o});var t=i(6540);const r={},s=t.createContext(r);function a(e){const n=t.useContext(s);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:a(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);