"use strict";(self.webpackChunkmy_notes=self.webpackChunkmy_notes||[]).push([[8628],{8182:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>u,frontMatter:()=>o,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"ml/supervised/neural-networks-regression","title":"Neural Networks for Regression","description":"Neural Networks for Regression are powerful supervised learning models that learn to predict continuous target variables by approximating complex, non-linear relationships between features and outputs.","source":"@site/docs/ml/supervised/neural-networks-regression.md","sourceDirName":"ml/supervised","slug":"/ml/supervised/neural-networks-regression","permalink":"/my-notes/docs/ml/supervised/neural-networks-regression","draft":false,"unlisted":false,"editUrl":"https://github.com/nvgr-GenAI/my-notes/edit/main/docs/ml/supervised/neural-networks-regression.md","tags":[],"version":"current","frontMatter":{},"sidebar":"mlSidebar","previous":{"title":"SVM Regression","permalink":"/my-notes/docs/ml/supervised/svm-regression"},"next":{"title":"Logistic Regression","permalink":"/my-notes/docs/ml/supervised/logistic-regression"}}');var t=r(4848),i=r(8453);const o={},a="Neural Networks for Regression",l={},d=[{value:"How Neural Networks Perform Regression",id:"how-neural-networks-perform-regression",level:2},{value:"Neural Network Architecture for Regression",id:"neural-network-architecture-for-regression",level:2},{value:"Key Components for Regression",id:"key-components-for-regression",level:2},{value:"Implementation: Single-output Regression Example",id:"implementation-single-output-regression-example",level:2},{value:"Implementation: Multi-output Regression Example",id:"implementation-multi-output-regression-example",level:2},{value:"Non-linear Regression with Deep Learning",id:"non-linear-regression-with-deep-learning",level:2},{value:"Time Series Regression with Neural Networks",id:"time-series-regression-with-neural-networks",level:2},{value:"Handling Outliers in Regression",id:"handling-outliers-in-regression",level:2},{value:"Hyperparameter Tuning for Regression",id:"hyperparameter-tuning-for-regression",level:2},{value:"Common Applications of Neural Networks for Regression",id:"common-applications-of-neural-networks-for-regression",level:2}];function c(e){const n={code:"code",h1:"h1",h2:"h2",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"neural-networks-for-regression",children:"Neural Networks for Regression"})}),"\n",(0,t.jsx)(n.p,{children:"Neural Networks for Regression are powerful supervised learning models that learn to predict continuous target variables by approximating complex, non-linear relationships between features and outputs."}),"\n",(0,t.jsx)(n.h2,{id:"how-neural-networks-perform-regression",children:"How Neural Networks Perform Regression"}),"\n",(0,t.jsx)(n.p,{children:"Neural networks for regression typically:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Process input features"})," through multiple hidden layers"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Transform data"})," using weights, biases, and activation functions"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Output continuous values"})," from a linear output layer (no activation function)"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Unlike classification networks that output probabilities, regression networks directly predict numerical values."}),"\n",(0,t.jsx)(n.h2,{id:"neural-network-architecture-for-regression",children:"Neural Network Architecture for Regression"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{src:"https://i.imgur.com/cRLLbIJ.png",alt:"Neural Network Regression Architecture"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Input Layer"}),": One neuron per feature in your dataset"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Hidden Layers"}),": Multiple layers with varying numbers of neurons that use non-linear activation functions"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Output Layer"}),": One or more neurons with no activation function (linear output) for continuous prediction"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"key-components-for-regression",children:"Key Components for Regression"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Loss Functions"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Mean Squared Error (MSE): Average of squared differences between predictions and targets"}),"\n",(0,t.jsx)(n.li,{children:"Mean Absolute Error (MAE): Average of absolute differences between predictions and targets"}),"\n",(0,t.jsx)(n.li,{children:"Huber Loss: Combines MSE and MAE, less sensitive to outliers"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Evaluation Metrics"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"MSE / RMSE (Root Mean Squared Error)"}),"\n",(0,t.jsx)(n.li,{children:"MAE (Mean Absolute Error)"}),"\n",(0,t.jsx)(n.li,{children:"R\xb2 (Coefficient of determination)"}),"\n",(0,t.jsx)(n.li,{children:"Explained variance"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Output Layer Design"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["For single target regression: ",(0,t.jsx)(n.code,{children:"Dense(1)"})," (no activation)"]}),"\n",(0,t.jsxs)(n.li,{children:["For multiple target regression: ",(0,t.jsx)(n.code,{children:"Dense(n)"})," (no activation)"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"implementation-single-output-regression-example",children:"Implementation: Single-output Regression Example"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import numpy as np\nfrom sklearn.datasets import make_regression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam\nimport matplotlib.pyplot as plt\n\n# Generate a sample regression dataset\nX, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=42)\n\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Scale the features\nscaler_X = StandardScaler()\nX_train_scaled = scaler_X.fit_transform(X_train)\nX_test_scaled = scaler_X.transform(X_test)\n\n# Scale the target\nscaler_y = StandardScaler()\ny_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()\ny_test_scaled = scaler_y.transform(y_test.reshape(-1, 1)).flatten()\n\n# Create a neural network for regression\nmodel = Sequential([\n    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n    Dropout(0.2),\n    Dense(32, activation='relu'),\n    Dropout(0.2),\n    Dense(16, activation='relu'),\n    Dense(1)  # No activation for regression output\n])\n\n# Compile the model\nmodel.compile(optimizer=Adam(learning_rate=0.001), \n              loss='mean_squared_error',\n              metrics=['mae'])\n\n# Display model architecture\nmodel.summary()\n\n# Train the model\nhistory = model.fit(X_train_scaled, y_train_scaled, \n                    epochs=50, \n                    batch_size=32, \n                    validation_split=0.2, \n                    verbose=1)\n\n# Plot training history\nplt.figure(figsize=(12, 5))\nplt.subplot(1, 2, 1)\nplt.plot(history.history['mae'], label='Training MAE')\nplt.plot(history.history['val_mae'], label='Validation MAE')\nplt.xlabel('Epochs')\nplt.ylabel('Mean Absolute Error')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(history.history['loss'], label='Training Loss (MSE)')\nplt.plot(history.history['val_loss'], label='Validation Loss (MSE)')\nplt.xlabel('Epochs')\nplt.ylabel('Mean Squared Error')\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n# Make predictions\ny_pred_scaled = model.predict(X_test_scaled).flatten()\ny_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()\n\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nr2 = r2_score(y_test, y_pred)\n\nprint(f\"Mean Squared Error: {mse:.4f}\")\nprint(f\"Root Mean Squared Error: {rmse:.4f}\")\nprint(f\"R-squared: {r2:.4f}\")\n\n# Plot predictions vs actual\nplt.figure(figsize=(10, 6))\nplt.scatter(y_test, y_pred, alpha=0.5)\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\nplt.xlabel('Actual Values')\nplt.ylabel('Predicted Values')\nplt.title('Neural Network Regression: Predicted vs Actual')\nplt.show()\n\n# Plot residuals\nresiduals = y_test - y_pred\nplt.figure(figsize=(10, 6))\nplt.scatter(y_pred, residuals, alpha=0.5)\nplt.hlines(y=0, xmin=y_pred.min(), xmax=y_pred.max(), colors='r', linestyles='--')\nplt.xlabel('Predicted Values')\nplt.ylabel('Residuals')\nplt.title('Residual Plot')\nplt.show()\n"})}),"\n",(0,t.jsx)(n.h2,{id:"implementation-multi-output-regression-example",children:"Implementation: Multi-output Regression Example"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# Generate multi-output regression dataset\nX_multi, y_multi = make_regression(n_samples=1000, n_features=20, n_targets=3, noise=0.1, random_state=42)\n\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(X_multi, y_multi, test_size=0.3, random_state=42)\n\n# Scale the features\nscaler_X = StandardScaler()\nX_train_scaled = scaler_X.fit_transform(X_train)\nX_test_scaled = scaler_X.transform(X_test)\n\n# Scale the targets\nscaler_y = StandardScaler()\ny_train_scaled = scaler_y.fit_transform(y_train)\ny_test_scaled = scaler_y.transform(y_test)\n\n# Create a neural network for multi-output regression\nmodel_multi = Sequential([\n    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n    Dropout(0.2),\n    Dense(32, activation='relu'),\n    Dropout(0.2),\n    Dense(16, activation='relu'),\n    Dense(3)  # Three outputs, no activation function\n])\n\n# Compile the model\nmodel_multi.compile(optimizer=Adam(learning_rate=0.001), \n                    loss='mean_squared_error', \n                    metrics=['mae'])\n\n# Display model architecture\nmodel_multi.summary()\n\n# Train the model\nhistory = model_multi.fit(X_train_scaled, y_train_scaled, \n                          epochs=50, \n                          batch_size=32, \n                          validation_split=0.2, \n                          verbose=1)\n\n# Make predictions\ny_pred_scaled = model_multi.predict(X_test_scaled)\ny_pred = scaler_y.inverse_transform(y_pred_scaled)\n\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nprint(f\"Mean Squared Error: {mse:.4f}\")\nprint(f\"Root Mean Squared Error: {rmse:.4f}\")\n\n# Evaluate individually for each output\nfor i in range(y_test.shape[1]):\n    r2 = r2_score(y_test[:, i], y_pred[:, i])\n    print(f\"Output {i+1} R-squared: {r2:.4f}\")\n"})}),"\n",(0,t.jsx)(n.h2,{id:"non-linear-regression-with-deep-learning",children:"Non-linear Regression with Deep Learning"}),"\n",(0,t.jsx)(n.p,{children:"Neural networks excel at capturing non-linear relationships that traditional regression models (like linear regression) cannot:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Complex Interactions"}),": Can model intricate interactions between features"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Feature Learning"}),": Automatically learn useful representations from raw data"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Flexible Modeling"}),": Can approximate virtually any continuous function"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"time-series-regression-with-neural-networks",children:"Time Series Regression with Neural Networks"}),"\n",(0,t.jsx)(n.p,{children:"For sequential data like time series:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"from tensorflow.keras.layers import LSTM\n\n# Create a sequence regression model with LSTM\nmodel_time_series = Sequential([\n    LSTM(64, return_sequences=True, input_shape=(sequence_length, features)),\n    Dropout(0.2),\n    LSTM(32),\n    Dense(16, activation='relu'),\n    Dense(1)  # Predict next value in sequence\n])\n\nmodel_time_series.compile(optimizer='adam', loss='mse', metrics=['mae'])\n"})}),"\n",(0,t.jsx)(n.h2,{id:"handling-outliers-in-regression",children:"Handling Outliers in Regression"}),"\n",(0,t.jsx)(n.p,{children:"Neural networks can be sensitive to outliers. Common strategies:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Robust Loss Functions"}),": Use Huber loss or log-cosh loss"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"model.compile(optimizer=Adam(learning_rate=0.001), \n              loss=tf.keras.losses.Huber(),\n              metrics=['mae'])\n"})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Data Preprocessing"}),": Remove or cap outliers before training"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# Identify and cap outliers\nq1, q3 = np.percentile(y_train, [25, 75])\niqr = q3 - q1\nlower_bound = q1 - (1.5 * iqr)\nupper_bound = q3 + (1.5 * iqr)\n\ny_train_capped = np.clip(y_train, lower_bound, upper_bound)\n"})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Quantile Regression"}),": Predict different quantiles instead of the mean"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"def quantile_loss(q, y_true, y_pred):\n    error = y_true - y_pred\n    return K.mean(K.maximum(q * error, (q - 1) * error), axis=-1)\n    \nmodel.compile(optimizer='adam', \n              loss=lambda y_true, y_pred: quantile_loss(0.5, y_true, y_pred),  # median\n              metrics=['mae'])\n"})}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"hyperparameter-tuning-for-regression",children:"Hyperparameter Tuning for Regression"}),"\n",(0,t.jsx)(n.p,{children:"Key hyperparameters to tune:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Network architecture"}),": Number of layers and neurons"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Learning rate"}),": Typically between 0.1 and 0.0001"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Batch size"}),": Common values: 16, 32, 64, 128"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Activation functions"}),": ReLU, LeakyReLU, ELU"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Regularization"}),": L1, L2, or both (ElasticNet)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Dropout rate"}),": Typically between 0.1 and 0.5"]}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# Example of hyperparameter tuning using RandomizedSearchCV\nfrom tensorflow.keras.wrappers.scikit_learn import KerasRegressor\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import uniform, randint\n\ndef create_model(learning_rate=0.001, neurons=64, dropout_rate=0.2):\n    model = Sequential([\n        Dense(neurons, activation='relu', input_shape=(X_train.shape[1],)),\n        Dropout(dropout_rate),\n        Dense(neurons // 2, activation='relu'),\n        Dropout(dropout_rate),\n        Dense(1)\n    ])\n    model.compile(optimizer=Adam(learning_rate=learning_rate), \n                 loss='mean_squared_error',\n                 metrics=['mae'])\n    return model\n\n# Create model\nkeras_regressor = KerasRegressor(build_fn=create_model, verbose=0)\n\n# Define parameter distribution\nparam_dist = {\n    'learning_rate': uniform(0.0001, 0.01),\n    'epochs': randint(30, 100),\n    'batch_size': randint(16, 128),\n    'neurons': randint(16, 128),\n    'dropout_rate': uniform(0.1, 0.4)\n}\n\n# Random search\nrandom_search = RandomizedSearchCV(estimator=keras_regressor, \n                                  param_distributions=param_dist,\n                                  n_iter=20, \n                                  cv=3,\n                                  scoring='neg_mean_squared_error')\nrandom_search.fit(X_train_scaled, y_train_scaled)\n\n# Print results\nprint(f\"Best: {-random_search.best_score_:.4f} MSE using {random_search.best_params_}\")\n"})}),"\n",(0,t.jsx)(n.h2,{id:"common-applications-of-neural-networks-for-regression",children:"Common Applications of Neural Networks for Regression"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"House price prediction"}),": Predicting property values based on features"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Stock market forecasting"}),": Predicting future stock prices"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Energy consumption prediction"}),": Forecasting electricity or gas usage"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Sales forecasting"}),": Predicting future sales volumes"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Medical outcome prediction"}),": Estimating patient recovery time or treatment effectiveness"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Environmental modeling"}),": Predicting pollution levels or weather patterns"]}),"\n"]})]})}function u(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>o,x:()=>a});var s=r(6540);const t={},i=s.createContext(t);function o(e){const n=s.useContext(i);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:o(e.components),s.createElement(i.Provider,{value:n},e.children)}}}]);