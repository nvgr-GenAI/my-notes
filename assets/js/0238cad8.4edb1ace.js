"use strict";(self.webpackChunkmy_notes=self.webpackChunkmy_notes||[]).push([[9912],{8453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>t});var r=i(6540);const l={},s=r.createContext(l);function o(e){const n=r.useContext(s);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function t(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(l):e.components||l:o(e.components),r.createElement(s.Provider,{value:n},e.children)}},9662:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>a,contentTitle:()=>t,default:()=>h,frontMatter:()=>o,metadata:()=>r,toc:()=>d});const r=JSON.parse('{"id":"ml/supervised/polynomial-regression","title":"Polynomial Regression","description":"Polynomial Regression is an extension of Linear Regression that allows modeling of non-linear relationships between dependent and independent variables. It fits a polynomial equation to the data by transforming the original features into polynomial features.","source":"@site/docs/ml/supervised/polynomial-regression.md","sourceDirName":"ml/supervised","slug":"/ml/supervised/polynomial-regression","permalink":"/my-notes/docs/ml/supervised/polynomial-regression","draft":false,"unlisted":false,"editUrl":"https://github.com/nvgr-GenAI/my-notes/edit/main/docs/ml/supervised/polynomial-regression.md","tags":[],"version":"current","frontMatter":{},"sidebar":"mlSidebar","previous":{"title":"Linear Regression","permalink":"/my-notes/docs/ml/supervised/linear-regression"},"next":{"title":"Ridge Regression","permalink":"/my-notes/docs/ml/supervised/ridge-regression"}}');var l=i(4848),s=i(8453);const o={},t="Polynomial Regression",a={},d=[{value:"How Polynomial Regression Works",id:"how-polynomial-regression-works",level:2},{value:"Advantages",id:"advantages",level:2},{value:"Disadvantages",id:"disadvantages",level:2},{value:"Implementation",id:"implementation",level:2},{value:"Choosing the Right Polynomial Degree",id:"choosing-the-right-polynomial-degree",level:2},{value:"Applications",id:"applications",level:2}];function p(e){const n={code:"code",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,l.jsxs)(l.Fragment,{children:[(0,l.jsx)(n.header,{children:(0,l.jsx)(n.h1,{id:"polynomial-regression",children:"Polynomial Regression"})}),"\n",(0,l.jsxs)(n.p,{children:["Polynomial Regression is an extension of ",(0,l.jsx)(n.strong,{children:"Linear Regression"})," that allows modeling of non-linear relationships between dependent and independent variables. It fits a polynomial equation to the data by transforming the original features into polynomial features."]}),"\n",(0,l.jsx)(n.h2,{id:"how-polynomial-regression-works",children:"How Polynomial Regression Works"}),"\n",(0,l.jsx)(n.p,{children:"Polynomial Regression transforms the original features into polynomial features and then applies standard linear regression. For example, with a single input variable x, a polynomial regression of degree 2 would create the model:"}),"\n",(0,l.jsx)(n.p,{children:"y = \u03b2\u2080 + \u03b2\u2081x + \u03b2\u2082x\xb2 + \u03b5"}),"\n",(0,l.jsx)(n.p,{children:"Where:"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"\u03b2\u2080, \u03b2\u2081, \u03b2\u2082 are the coefficients"}),"\n",(0,l.jsx)(n.li,{children:"x is the independent variable"}),"\n",(0,l.jsx)(n.li,{children:"\u03b5 is the error term"}),"\n"]}),"\n",(0,l.jsx)(n.h2,{id:"advantages",children:"Advantages"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"Can capture non-linear relationships"}),"\n",(0,l.jsx)(n.li,{children:"Easy to understand and implement"}),"\n",(0,l.jsx)(n.li,{children:"More flexible than simple linear regression"}),"\n",(0,l.jsx)(n.li,{children:"Based on the well-established linear regression framework"}),"\n",(0,l.jsx)(n.li,{children:"Useful for modeling curved relationships"}),"\n"]}),"\n",(0,l.jsx)(n.h2,{id:"disadvantages",children:"Disadvantages"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"Prone to overfitting, especially with high-degree polynomials"}),"\n",(0,l.jsx)(n.li,{children:"Sensitive to outliers"}),"\n",(0,l.jsx)(n.li,{children:"Less interpretable with higher degrees"}),"\n",(0,l.jsx)(n.li,{children:"May perform poorly on extrapolation"}),"\n",(0,l.jsx)(n.li,{children:"Computationally expensive with many features due to combinatorial explosion"}),"\n"]}),"\n",(0,l.jsx)(n.h2,{id:"implementation",children:"Implementation"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# Generate sample data\nnp.random.seed(42)\nX = np.sort(5 * np.random.rand(100, 1), axis=0)\ny = np.sin(X).ravel() + np.random.normal(0, 0.1, X.shape[0])\n\n# Create polynomial regression models with different degrees\ndegrees = [1, 2, 3, 5]\nplt.figure(figsize=(14, 8))\n\nfor i, degree in enumerate(degrees):\n    # Create polynomial regression pipeline\n    model = Pipeline([\n        ('poly', PolynomialFeatures(degree=degree)),\n        ('linear', LinearRegression())\n    ])\n    \n    # Fit the model\n    model.fit(X, y)\n    \n    # Make predictions on a fine-grained scale for plotting\n    X_test = np.linspace(0, 5, 100)[:, np.newaxis]\n    y_pred = model.predict(X_test)\n    \n    # Calculate metrics\n    y_train_pred = model.predict(X)\n    mse = mean_squared_error(y, y_train_pred)\n    r2 = r2_score(y, y_train_pred)\n    \n    # Plot the results\n    plt.subplot(2, 2, i+1)\n    plt.scatter(X, y, color='blue', s=30, alpha=0.5, label='Data points')\n    plt.plot(X_test, y_pred, color='red', label=f'Degree {degree}')\n    plt.title(f'Polynomial Degree {degree}\\nMSE = {mse:.4f}, R\xb2 = {r2:.4f}')\n    plt.legend()\n    plt.xlabel('X')\n    plt.ylabel('y')\n\nplt.tight_layout()\nplt.show()\n"})}),"\n",(0,l.jsx)(n.h2,{id:"choosing-the-right-polynomial-degree",children:"Choosing the Right Polynomial Degree"}),"\n",(0,l.jsx)(n.p,{children:"The degree of the polynomial is a crucial hyperparameter:"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Low degree (1-2)"}),": May underfit if the relationship is complex"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Medium degree (3-5)"}),": Often provides a good balance for many real-world applications"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"High degree (>5)"}),": Likely to overfit the training data"]}),"\n"]}),"\n",(0,l.jsx)(n.p,{children:"Use techniques like cross-validation or information criteria (AIC, BIC) to select the optimal degree."}),"\n",(0,l.jsx)(n.h2,{id:"applications",children:"Applications"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"Economic trend analysis"}),"\n",(0,l.jsx)(n.li,{children:"Growth curve modeling"}),"\n",(0,l.jsx)(n.li,{children:"Physical phenomena with known polynomial behavior"}),"\n",(0,l.jsx)(n.li,{children:"Signal processing"}),"\n",(0,l.jsx)(n.li,{children:"Machine learning feature engineering"}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,l.jsx)(n,{...e,children:(0,l.jsx)(p,{...e})}):p(e)}}}]);