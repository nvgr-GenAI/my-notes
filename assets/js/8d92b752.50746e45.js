"use strict";(self.webpackChunkmy_notes=self.webpackChunkmy_notes||[]).push([[6782],{6475:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>m,frontMatter:()=>o,metadata:()=>r,toc:()=>d});const r=JSON.parse('{"id":"ml/supervised/random-forest-regression","title":"Random Forest Regression","description":"Random Forest Regression is an ensemble learning method that combines multiple decision trees to predict continuous target variables. It\'s a powerful technique that improves accuracy and reduces overfitting compared to single decision trees.","source":"@site/docs/ml/supervised/random-forest-regression.md","sourceDirName":"ml/supervised","slug":"/ml/supervised/random-forest-regression","permalink":"/my-notes/docs/ml/supervised/random-forest-regression","draft":false,"unlisted":false,"editUrl":"https://github.com/nvgr-GenAI/my-notes/edit/main/docs/ml/supervised/random-forest-regression.md","tags":[],"version":"current","frontMatter":{},"sidebar":"mlSidebar","previous":{"title":"Decision Tree Regression","permalink":"/my-notes/docs/ml/supervised/decision-tree-regression"},"next":{"title":"SVM Regression","permalink":"/my-notes/docs/ml/supervised/svm-regression"}}');var t=s(4848),i=s(8453);const o={},a="Random Forest Regression",l={},d=[{value:"How Random Forest Regression Works",id:"how-random-forest-regression-works",level:2},{value:"Advantages",id:"advantages",level:2},{value:"Disadvantages",id:"disadvantages",level:2},{value:"Implementation",id:"implementation",level:2},{value:"Hyperparameters",id:"hyperparameters",level:2},{value:"Applications",id:"applications",level:2}];function c(e){const n={code:"code",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"random-forest-regression",children:"Random Forest Regression"})}),"\n",(0,t.jsxs)(n.p,{children:["Random Forest Regression is an ",(0,t.jsx)(n.strong,{children:"ensemble learning method"})," that combines multiple decision trees to predict continuous target variables. It's a powerful technique that improves accuracy and reduces overfitting compared to single decision trees."]}),"\n",(0,t.jsx)(n.h2,{id:"how-random-forest-regression-works",children:"How Random Forest Regression Works"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Bootstrap Aggregating (Bagging)"}),": Creates multiple subsets of the training data with replacement."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Decision Tree Creation"}),": Builds a regression decision tree for each subset, using a random subset of features at each split."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Averaging"}),": For regression, the final prediction is the average of predictions from all trees."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"advantages",children:"Advantages"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Reduces overfitting compared to individual decision trees"}),"\n",(0,t.jsx)(n.li,{children:"Handles high-dimensional data effectively"}),"\n",(0,t.jsx)(n.li,{children:"Provides feature importance measures"}),"\n",(0,t.jsx)(n.li,{children:"Robust to outliers"}),"\n",(0,t.jsx)(n.li,{children:"Can handle non-linear relationships well"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"disadvantages",children:"Disadvantages"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Less interpretable than a single decision tree"}),"\n",(0,t.jsx)(n.li,{children:"Computationally expensive for large datasets"}),"\n",(0,t.jsx)(n.li,{children:"May overfit on noisy datasets"}),"\n",(0,t.jsx)(n.li,{children:"Not as effective for extrapolation (predictions outside the range of training data)"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"implementation",children:"Implementation"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.datasets import make_regression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport numpy as np\n\n# Generate a sample dataset\nX, y = make_regression(n_samples=1000, n_features=10, noise=0.1, random_state=42)\n\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Create and train the model\nrf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)\nrf_regressor.fit(X_train, y_train)\n\n# Make predictions\ny_pred = rf_regressor.predict(X_test)\n\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nr2 = r2_score(y_test, y_pred)\n\nprint(f"Mean Squared Error: {mse:.4f}")\nprint(f"Root Mean Squared Error: {rmse:.4f}")\nprint(f"R-squared: {r2:.4f}")\n\n# Feature importance\nfeature_importance = pd.DataFrame({\n    \'Feature\': range(X.shape[1]),\n    \'Importance\': rf_regressor.feature_importances_\n}).sort_values(\'Importance\', ascending=False)\nprint("\\nFeature Importance:")\nprint(feature_importance.head(10))\n'})}),"\n",(0,t.jsx)(n.h2,{id:"hyperparameters",children:"Hyperparameters"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"n_estimators"}),": Number of trees in the forest"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"max_depth"}),": Maximum depth of each tree"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"min_samples_split"}),": Minimum samples required to split a node"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"min_samples_leaf"}),": Minimum samples required in a leaf node"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"max_features"}),": Number of features to consider for the best split"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"applications",children:"Applications"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Housing price prediction"}),"\n",(0,t.jsx)(n.li,{children:"Stock price forecasting"}),"\n",(0,t.jsx)(n.li,{children:"Sales forecasting"}),"\n",(0,t.jsx)(n.li,{children:"Energy consumption prediction"}),"\n",(0,t.jsx)(n.li,{children:"Weather forecasting"}),"\n"]})]})}function m(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>o,x:()=>a});var r=s(6540);const t={},i=r.createContext(t);function o(e){const n=r.useContext(i);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:o(e.components),r.createElement(i.Provider,{value:n},e.children)}}}]);