"use strict";(self.webpackChunkmy_notes=self.webpackChunkmy_notes||[]).push([[6748],{7602:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>a,contentTitle:()=>l,default:()=>d,frontMatter:()=>o,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"ml/reinforcement/examples","title":"Examples of Reinforcement Learning","description":"Example 1: Game Playing","source":"@site/docs/ml/reinforcement/examples.md","sourceDirName":"ml/reinforcement","slug":"/ml/reinforcement/examples","permalink":"/my-notes/docs/ml/reinforcement/examples","draft":false,"unlisted":false,"editUrl":"https://github.com/nvgr-GenAI/my-notes/edit/main/docs/ml/reinforcement/examples.md","tags":[],"version":"current","frontMatter":{"title":"Examples of Reinforcement Learning"},"sidebar":"mlSidebar","previous":{"title":"Introduction to Reinforcement Learning","permalink":"/my-notes/docs/ml/reinforcement/intro"},"next":{"title":"ML Tools and Frameworks","permalink":"/my-notes/docs/ml/tools-frameworks"}}');var i=t(4848),s=t(8453);const o={title:"Examples of Reinforcement Learning"},l="Examples of Reinforcement Learning",a={},c=[{value:"Example 1: Game Playing",id:"example-1-game-playing",level:2},{value:"Example 2: Robotics",id:"example-2-robotics",level:2},{value:"Example 3: Autonomous Driving",id:"example-3-autonomous-driving",level:2}];function m(e){const n={h1:"h1",h2:"h2",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"examples-of-reinforcement-learning",children:"Examples of Reinforcement Learning"})}),"\n",(0,i.jsx)(n.h2,{id:"example-1-game-playing",children:"Example 1: Game Playing"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Input"}),": Game state"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Output"}),": Optimal moves"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Algorithm"}),": Deep Q-Networks (DQN)"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"example-2-robotics",children:"Example 2: Robotics"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Input"}),": Sensor data"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Output"}),": Motor commands"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Algorithm"}),": Policy Gradient Methods"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"example-3-autonomous-driving",children:"Example 3: Autonomous Driving"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Input"}),": Road conditions"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Output"}),": Steering and acceleration"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Algorithm"}),": Actor-Critic Methods"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"These examples showcase the potential of reinforcement learning in dynamic and interactive environments."})]})}function d(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(m,{...e})}):m(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>l});var r=t(6540);const i={},s=r.createContext(i);function o(e){const n=r.useContext(s);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),r.createElement(s.Provider,{value:n},e.children)}}}]);