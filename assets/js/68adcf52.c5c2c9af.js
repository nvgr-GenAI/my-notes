"use strict";(self.webpackChunkmy_notes=self.webpackChunkmy_notes||[]).push([[9560],{3944:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>c,contentTitle:()=>l,default:()=>h,frontMatter:()=>t,metadata:()=>i,toc:()=>o});const i=JSON.parse('{"id":"ml/supervised/svm-classification","title":"SVM Classification","description":"Support Vector Machine (SVM) Classification is a powerful supervised learning algorithm that finds the optimal hyperplane to separate different classes of data points. It\'s particularly effective for complex classification tasks in high-dimensional spaces.","source":"@site/docs/ml/supervised/svm-classification.md","sourceDirName":"ml/supervised","slug":"/ml/supervised/svm-classification","permalink":"/my-notes/docs/ml/supervised/svm-classification","draft":false,"unlisted":false,"editUrl":"https://github.com/nvgr-GenAI/my-notes/edit/main/docs/ml/supervised/svm-classification.md","tags":[],"version":"current","frontMatter":{},"sidebar":"mlSidebar","previous":{"title":"K-Nearest Neighbors","permalink":"/my-notes/docs/ml/supervised/k-nearest-neighbors"},"next":{"title":"Neural Networks for Classification","permalink":"/my-notes/docs/ml/supervised/neural-networks-classification"}}');var r=s(4848),a=s(8453);const t={},l="SVM Classification",c={},o=[{value:"How SVM Classification Works",id:"how-svm-classification-works",level:2},{value:"Types of SVM Classification",id:"types-of-svm-classification",level:2},{value:"Advantages",id:"advantages",level:2},{value:"Disadvantages",id:"disadvantages",level:2},{value:"Implementation",id:"implementation",level:2},{value:"Kernel Functions",id:"kernel-functions",level:2},{value:"Hyperparameters",id:"hyperparameters",level:2},{value:"Applications",id:"applications",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"svm-classification",children:"SVM Classification"})}),"\n",(0,r.jsxs)(n.p,{children:["Support Vector Machine (SVM) Classification is a powerful ",(0,r.jsx)(n.strong,{children:"supervised learning algorithm"})," that finds the optimal hyperplane to separate different classes of data points. It's particularly effective for complex classification tasks in high-dimensional spaces."]}),"\n",(0,r.jsx)(n.h2,{id:"how-svm-classification-works",children:"How SVM Classification Works"}),"\n",(0,r.jsx)(n.p,{children:"SVM classification works by finding the hyperplane that maximizes the margin between classes. The key concepts include:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Optimal Hyperplane"}),": The decision boundary that maximizes the margin between classes"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Support Vectors"}),": The data points closest to the hyperplane that influence its position and orientation"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Margin"}),": The distance between the hyperplane and the closest data points from each class"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Kernel Trick"}),": A method to handle non-linearly separable data by mapping it to a higher-dimensional space"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"types-of-svm-classification",children:"Types of SVM Classification"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Linear SVM"}),": Uses a linear decision boundary"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Non-linear SVM"}),": Uses kernel functions (polynomial, RBF, sigmoid) for complex decision boundaries"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Multi-class SVM"}),": Extends the binary classification to multiple classes using either one-vs-rest or one-vs-one approaches"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"advantages",children:"Advantages"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Effective in high-dimensional spaces"}),"\n",(0,r.jsx)(n.li,{children:"Memory efficient (uses only support vectors for the decision function)"}),"\n",(0,r.jsx)(n.li,{children:"Versatile due to different kernel functions"}),"\n",(0,r.jsx)(n.li,{children:"Works well with clear margin of separation"}),"\n",(0,r.jsx)(n.li,{children:"Robust against overfitting, especially in high-dimensional spaces"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"disadvantages",children:"Disadvantages"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Not suitable for large datasets (computationally intensive)"}),"\n",(0,r.jsx)(n.li,{children:"Performs poorly with overlapping classes"}),"\n",(0,r.jsx)(n.li,{children:"Sensitive to noise"}),"\n",(0,r.jsx)(n.li,{children:"Not directly probabilistic (requires adjustments for probability estimates)"}),"\n",(0,r.jsx)(n.li,{children:"Requires careful tuning of hyperparameters"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"implementation",children:"Implementation"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from sklearn import svm\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Generate a simple dataset for visualization\nX, y = make_classification(n_samples=100, n_features=2, n_informative=2, n_redundant=0, \n                           n_clusters_per_class=1, random_state=42)\n\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Scale the features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Create and train SVM models with different kernels\nkernels = ['linear', 'poly', 'rbf', 'sigmoid']\nplt.figure(figsize=(15, 10))\n\nfor i, kernel in enumerate(kernels):\n    # Create and train the model\n    svm_model = svm.SVC(kernel=kernel, gamma='auto')\n    svm_model.fit(X_train_scaled, y_train)\n    \n    # Make predictions\n    y_pred = svm_model.predict(X_test_scaled)\n    \n    # Evaluate the model\n    accuracy = accuracy_score(y_test, y_pred)\n    \n    # Plot decision boundary\n    plt.subplot(2, 2, i+1)\n    \n    # Create a mesh grid\n    h = 0.02  # step size in the mesh\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n    \n    # Scale the mesh grid\n    mesh_points = np.c_[xx.ravel(), yy.ravel()]\n    mesh_points_scaled = scaler.transform(mesh_points)\n    \n    # Plot the decision boundary\n    Z = svm_model.predict(mesh_points_scaled)\n    Z = Z.reshape(xx.shape)\n    plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n    \n    # Plot the training points\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm, edgecolors='k')\n    plt.xlabel('Feature 1')\n    plt.ylabel('Feature 2')\n    plt.title(f'SVM with {kernel} kernel\\nAccuracy: {accuracy:.4f}')\n\nplt.tight_layout()\nplt.show()\n\n# Fine-tuning SVM with GridSearchCV\nparam_grid = {\n    'C': [0.1, 1, 10, 100],\n    'gamma': ['scale', 'auto', 0.1, 0.01, 0.001],\n    'kernel': ['rbf']\n}\n\ngrid_search = GridSearchCV(svm.SVC(), param_grid, cv=5, scoring='accuracy', verbose=1)\ngrid_search.fit(X_train_scaled, y_train)\n\nprint(f\"Best parameters: {grid_search.best_params_}\")\nprint(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")\n\n# Train final model with best parameters\nbest_svm = grid_search.best_estimator_\ny_pred = best_svm.predict(X_test_scaled)\n\n# Evaluate final model\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Final model accuracy: {accuracy:.4f}\")\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred))\nprint(\"\\nConfusion Matrix:\")\nprint(confusion_matrix(y_test, y_pred))\n"})}),"\n",(0,r.jsx)(n.h2,{id:"kernel-functions",children:"Kernel Functions"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Linear"}),":\n",(0,r.jsxs)("div",{class:"math",children:["K(x",(0,r.jsx)("sub",{children:"i"}),", x",(0,r.jsx)("sub",{children:"j"}),") = x",(0,r.jsx)("sub",{children:"i"}),(0,r.jsx)("sup",{children:"T"})," x",(0,r.jsx)("sub",{children:"j"})]})]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Suitable for linearly separable data"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Polynomial"}),":\n",(0,r.jsxs)("div",{class:"math",children:["K(x",(0,r.jsx)("sub",{children:"i"}),", x",(0,r.jsx)("sub",{children:"j"}),") = (\u03b3x",(0,r.jsx)("sub",{children:"i"}),(0,r.jsx)("sup",{children:"T"})," x",(0,r.jsx)("sub",{children:"j"})," + r)",(0,r.jsx)("sup",{children:"d"})]})]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Good for non-linear boundaries"}),"\n",(0,r.jsx)(n.li,{children:"Parameters: degree d, gamma \u03b3, and coefficient r"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"RBF (Radial Basis Function)"}),":\n",(0,r.jsxs)("div",{class:"math",children:["K(x",(0,r.jsx)("sub",{children:"i"}),", x",(0,r.jsx)("sub",{children:"j"}),") = exp(-\u03b3||x",(0,r.jsx)("sub",{children:"i"})," - x",(0,r.jsx)("sub",{children:"j"}),"||",(0,r.jsx)("sup",{children:"2"}),")"]})]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Most commonly used kernel"}),"\n",(0,r.jsx)(n.li,{children:"Works well for most datasets"}),"\n",(0,r.jsx)(n.li,{children:"Parameter: gamma \u03b3 controls the influence of training examples"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Sigmoid"}),":\n",(0,r.jsxs)("div",{class:"math",children:["K(x",(0,r.jsx)("sub",{children:"i"}),", x",(0,r.jsx)("sub",{children:"j"}),") = tanh(\u03b3x",(0,r.jsx)("sub",{children:"i"}),(0,r.jsx)("sup",{children:"T"})," x",(0,r.jsx)("sub",{children:"j"})," + r)"]})]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Related to neural networks"}),"\n",(0,r.jsx)(n.li,{children:"Parameters: gamma \u03b3 and coefficient r"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"hyperparameters",children:"Hyperparameters"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"C"}),": Regularization parameter. Lower values = more regularization"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"gamma"}),": Kernel coefficient for 'rbf', 'poly', and 'sigmoid'"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"kernel"}),": Type of kernel function"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"degree"}),": Degree of polynomial kernel function"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"coef0"}),": Independent term in kernel function"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"applications",children:"Applications"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Image classification"}),"\n",(0,r.jsx)(n.li,{children:"Text categorization"}),"\n",(0,r.jsx)(n.li,{children:"Bioinformatics (protein classification, cancer classification)"}),"\n",(0,r.jsx)(n.li,{children:"Face detection"}),"\n",(0,r.jsx)(n.li,{children:"Handwriting recognition"}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>t,x:()=>l});var i=s(6540);const r={},a=i.createContext(r);function t(e){const n=i.useContext(a);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:t(e.components),i.createElement(a.Provider,{value:n},e.children)}}}]);