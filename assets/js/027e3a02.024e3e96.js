"use strict";(self.webpackChunkmy_notes=self.webpackChunkmy_notes||[]).push([[4087],{4908:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>o,contentTitle:()=>l,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"ml/supervised/k-nearest-neighbors","title":"K-Nearest Neighbors","description":"Understanding the K-Nearest Neighbors (KNN) algorithm for classification and regression","source":"@site/docs/ml/supervised/k-nearest-neighbors.md","sourceDirName":"ml/supervised","slug":"/ml/supervised/k-nearest-neighbors","permalink":"/my-notes/docs/ml/supervised/k-nearest-neighbors","draft":false,"unlisted":false,"editUrl":"https://github.com/nvgr-GenAI/my-notes/edit/main/docs/ml/supervised/k-nearest-neighbors.md","tags":[],"version":"current","sidebarPosition":8,"frontMatter":{"title":"K-Nearest Neighbors","sidebar_position":8,"description":"Understanding the K-Nearest Neighbors (KNN) algorithm for classification and regression"},"sidebar":"mlSidebar","previous":{"title":"Naive Bayes","permalink":"/my-notes/docs/ml/supervised/naive-bayes"},"next":{"title":"SVM Classification","permalink":"/my-notes/docs/ml/supervised/svm-classification"}}');var t=s(4848),a=s(8453);const r={title:"K-Nearest Neighbors",sidebar_position:8,description:"Understanding the K-Nearest Neighbors (KNN) algorithm for classification and regression"},l="K-Nearest Neighbors (KNN)",o={},c=[{value:"1. Types of K-Nearest Neighbors",id:"1-types-of-k-nearest-neighbors",level:2},{value:"A. KNN for Classification",id:"a-knn-for-classification",level:3},{value:"B. KNN for Regression",id:"b-knn-for-regression",level:3},{value:"2. How KNN Works",id:"2-how-knn-works",level:2},{value:"Distance Metrics:",id:"distance-metrics",level:3},{value:"Choosing K:",id:"choosing-k",level:3},{value:"3. Example Use Case: Iris Flower Classification",id:"3-example-use-case-iris-flower-classification",level:2},{value:"Scenario",id:"scenario",level:3},{value:"Dataset Sample",id:"dataset-sample",level:3},{value:"KNN Approach",id:"knn-approach",level:3},{value:"4. Advantages of KNN",id:"4-advantages-of-knn",level:2},{value:"5. Limitations",id:"5-limitations",level:2},{value:"6. Real-World Applications",id:"6-real-world-applications",level:2},{value:"7. Implementation Example",id:"7-implementation-example",level:2},{value:"Summary",id:"summary",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,a.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"k-nearest-neighbors-knn",children:"K-Nearest Neighbors (KNN)"})}),"\n",(0,t.jsxs)(n.p,{children:["K-Nearest Neighbors is a simple, versatile, and intuitive ",(0,t.jsx)(n.strong,{children:"supervised learning algorithm"})," used for both classification and regression. It works on the principle that similar data points exist in close proximity to each other."]}),"\n",(0,t.jsx)(n.h2,{id:"1-types-of-k-nearest-neighbors",children:"1. Types of K-Nearest Neighbors"}),"\n",(0,t.jsx)(n.h3,{id:"a-knn-for-classification",children:"A. KNN for Classification"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Predicts the class of a data point based on the majority class among its K nearest neighbors"}),"\n",(0,t.jsx)(n.li,{children:"Example: Classifying whether an email is spam based on similarities to known emails"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"b-knn-for-regression",children:"B. KNN for Regression"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Predicts a continuous value by averaging the values of its K nearest neighbors"}),"\n",(0,t.jsx)(n.li,{children:"Example: Predicting house prices based on similar houses in the neighborhood"}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"2-how-knn-works",children:"2. How KNN Works"}),"\n",(0,t.jsxs)(n.p,{children:["KNN is a ",(0,t.jsx)(n.strong,{children:"lazy learning"})," algorithm, meaning it doesn't build a model during training but memorizes the training data. The algorithm follows these steps:"]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Store"})," all training examples with their labels"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Calculate"})," the distance between a new example and all training examples"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Select"})," the K-nearest examples based on the calculated distances"]}),"\n",(0,t.jsxs)(n.li,{children:["For ",(0,t.jsx)(n.strong,{children:"classification"}),": Take a majority vote among the K neighbors"]}),"\n",(0,t.jsxs)(n.li,{children:["For ",(0,t.jsx)(n.strong,{children:"regression"}),": Calculate the average value among the K neighbors"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"distance-metrics",children:"Distance Metrics:"}),"\n",(0,t.jsx)(n.p,{children:"Different distance measures can be used:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Euclidean Distance"})," (most common): \u221a(\u03a3(x\u1d62 - y\u1d62)\xb2)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Manhattan Distance"}),": \u03a3|x\u1d62 - y\u1d62|"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Minkowski Distance"}),": (\u03a3|x\u1d62 - y\u1d62|\u1d56)^(1/p)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Hamming Distance"}),": Count of positions where corresponding values differ (for categorical data)"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"choosing-k",children:"Choosing K:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Small K"}),": More sensitive to noise, more flexible decision boundary"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Large K"}),": Smoother decision boundary, less prone to overfitting"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Odd K"}),": Recommended for binary classification to avoid ties"]}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"3-example-use-case-iris-flower-classification",children:"3. Example Use Case: Iris Flower Classification"}),"\n",(0,t.jsx)(n.h3,{id:"scenario",children:"Scenario"}),"\n",(0,t.jsx)(n.p,{children:"A botanist wants to classify iris flowers into species based on measurements of the sepal and petal dimensions."}),"\n",(0,t.jsx)(n.h3,{id:"dataset-sample",children:"Dataset Sample"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Sepal Length (cm)"}),(0,t.jsx)(n.th,{children:"Sepal Width (cm)"}),(0,t.jsx)(n.th,{children:"Petal Length (cm)"}),(0,t.jsx)(n.th,{children:"Petal Width (cm)"}),(0,t.jsx)(n.th,{children:"Species"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"5.1"}),(0,t.jsx)(n.td,{children:"3.5"}),(0,t.jsx)(n.td,{children:"1.4"}),(0,t.jsx)(n.td,{children:"0.2"}),(0,t.jsx)(n.td,{children:"Setosa"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"7.0"}),(0,t.jsx)(n.td,{children:"3.2"}),(0,t.jsx)(n.td,{children:"4.7"}),(0,t.jsx)(n.td,{children:"1.4"}),(0,t.jsx)(n.td,{children:"Versicolor"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"6.3"}),(0,t.jsx)(n.td,{children:"3.3"}),(0,t.jsx)(n.td,{children:"6.0"}),(0,t.jsx)(n.td,{children:"2.5"}),(0,t.jsx)(n.td,{children:"Virginica"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"4.9"}),(0,t.jsx)(n.td,{children:"3.0"}),(0,t.jsx)(n.td,{children:"1.4"}),(0,t.jsx)(n.td,{children:"0.2"}),(0,t.jsx)(n.td,{children:"Setosa"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"6.4"}),(0,t.jsx)(n.td,{children:"3.2"}),(0,t.jsx)(n.td,{children:"4.5"}),(0,t.jsx)(n.td,{children:"1.5"}),(0,t.jsx)(n.td,{children:"Versicolor"})]})]})]}),"\n",(0,t.jsx)(n.h3,{id:"knn-approach",children:"KNN Approach"}),"\n",(0,t.jsx)(n.p,{children:"Let's classify a new flower with measurements:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Sepal Length: 5.8 cm"}),"\n",(0,t.jsx)(n.li,{children:"Sepal Width: 2.7 cm"}),"\n",(0,t.jsx)(n.li,{children:"Petal Length: 4.1 cm"}),"\n",(0,t.jsx)(n.li,{children:"Petal Width: 1.0 cm"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Steps:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Calculate distances from the new flower to all flowers in the dataset"}),"\n",(0,t.jsx)(n.li,{children:"With K=3, select the 3 closest flowers based on distance"}),"\n",(0,t.jsx)(n.li,{children:"Take a majority vote of the species from these 3 neighbors"}),"\n",(0,t.jsx)(n.li,{children:"Assign the majority species to the new flower"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"In this example, if 2 of the 3 nearest neighbors are Versicolor and 1 is Setosa, the new flower is classified as Versicolor."}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"4-advantages-of-knn",children:"4. Advantages of KNN"}),"\n",(0,t.jsx)(n.p,{children:"\u2705 Simple and intuitive algorithm"}),"\n",(0,t.jsx)(n.p,{children:"\u2705 No training phase (lazy learning)"}),"\n",(0,t.jsx)(n.p,{children:"\u2705 Works for both classification and regression"}),"\n",(0,t.jsx)(n.p,{children:"\u2705 No assumptions about data distribution (non-parametric)"}),"\n",(0,t.jsx)(n.p,{children:"\u2705 Naturally handles multi-class classification"}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"5-limitations",children:"5. Limitations"}),"\n",(0,t.jsx)(n.p,{children:"\u274c Computationally expensive for large datasets (calculates distances to all points)"}),"\n",(0,t.jsx)(n.p,{children:"\u274c Requires feature scaling for accurate results"}),"\n",(0,t.jsx)(n.p,{children:"\u274c Struggles with high-dimensional data (curse of dimensionality)"}),"\n",(0,t.jsx)(n.p,{children:"\u274c Sensitive to noisy data and outliers"}),"\n",(0,t.jsx)(n.p,{children:"\u274c Imbalanced data can bias predictions toward the majority class"}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"6-real-world-applications",children:"6. Real-World Applications"}),"\n",(0,t.jsxs)(n.p,{children:["\ud83d\udcf1 ",(0,t.jsx)(n.strong,{children:"Recommendation Systems"}),' \u2013 "Customers who bought this also bought..."']}),"\n",(0,t.jsxs)(n.p,{children:["\ud83c\udfb5 ",(0,t.jsx)(n.strong,{children:"Music Genre Classification"})," \u2013 Categorizing songs based on audio features"]}),"\n",(0,t.jsxs)(n.p,{children:["\ud83c\udfe5 ",(0,t.jsx)(n.strong,{children:"Medical Diagnosis"})," \u2013 Classifying diseases based on symptom proximity to known cases"]}),"\n",(0,t.jsxs)(n.p,{children:["\ud83d\udcb3 ",(0,t.jsx)(n.strong,{children:"Credit Scoring"})," \u2013 Assessing creditworthiness based on similar applicants"]}),"\n",(0,t.jsxs)(n.p,{children:["\ud83c\udfae ",(0,t.jsx)(n.strong,{children:"Computer Vision"})," \u2013 Image recognition and object detection"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"7-implementation-example",children:"7. Implementation Example"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# Install Dependencies\n# pip install numpy pandas scikit-learn matplotlib seaborn\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\n# Load the Iris dataset\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\nfeature_names = iris.feature_names\ntarget_names = iris.target_names\n\n# Create a DataFrame for easier data manipulation and visualization\ndf = pd.DataFrame(X, columns=feature_names)\ndf['species'] = pd.Categorical.from_codes(y, target_names)\n\n# Display the first few rows\nprint(df.head())\n\n# Visualize the data\nplt.figure(figsize=(15, 10))\n\n# Pairplot to see relationships between features\nsns.pairplot(df, hue='species', markers=[\"o\", \"s\", \"D\"])\nplt.suptitle(\"Iris Dataset - Feature Relationships by Species\", y=1.02)\nplt.show()\n\n# Scatter plot of the most discriminative features\nplt.figure(figsize=(12, 6))\n\nplt.subplot(1, 2, 1)\nsns.scatterplot(x=df.iloc[:, 0], y=df.iloc[:, 1], hue=df['species'], style=df['species'], s=70)\nplt.xlabel(feature_names[0])\nplt.ylabel(feature_names[1])\nplt.title('Iris Species by Sepal Dimensions')\n\nplt.subplot(1, 2, 2)\nsns.scatterplot(x=df.iloc[:, 2], y=df.iloc[:, 3], hue=df['species'], style=df['species'], s=70)\nplt.xlabel(feature_names[2])\nplt.ylabel(feature_names[3])\nplt.title('Iris Species by Petal Dimensions')\n\nplt.tight_layout()\nplt.show()\n\n# Standardize the features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n\n# Finding the optimal K value\nk_values = list(range(1, 31))\ncv_scores = []\n\nfor k in k_values:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    scores = cross_val_score(knn, X_train, y_train, cv=5, scoring='accuracy')\n    cv_scores.append(scores.mean())\n\n# Plot the CV accuracy vs. K Value\nplt.figure(figsize=(10, 6))\nplt.plot(k_values, cv_scores, 'o-')\nplt.xlabel('Number of Neighbors (K)')\nplt.ylabel('Cross-Validation Accuracy')\nplt.title('Finding Optimal K Value')\nplt.grid(True)\nplt.show()\n\n# Get the optimal K value\noptimal_k = k_values[cv_scores.index(max(cv_scores))]\nprint(f\"The optimal number of neighbors is {optimal_k} with accuracy: {max(cv_scores):.4f}\")\n\n# Train the model with the optimal K\nknn = KNeighborsClassifier(n_neighbors=optimal_k)\nknn.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred = knn.predict(X_test)\n\n# Evaluate the model\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"\\nAccuracy: {accuracy:.4f}\")\n\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred, target_names=target_names))\n\nprint(\"\\nConfusion Matrix:\")\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot=True, cmap='Blues', xticklabels=target_names, yticklabels=target_names)\nplt.xlabel('Predicted Species')\nplt.ylabel('Actual Species')\nplt.show()\n\n# Visualize the decision boundaries (using only 2 features for simplicity)\ndef plot_decision_boundaries(X, y, model, feature_idx=(2, 3)):\n    # Extract the two features we want to visualize\n    X_vis = X[:, feature_idx]\n    \n    # Create a mesh grid\n    h = 0.02  # Step size\n    x_min, x_max = X_vis[:, 0].min() - 1, X_vis[:, 0].max() + 1\n    y_min, y_max = X_vis[:, 1].min() - 1, X_vis[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n    \n    # Create a test dataset containing only the two selected features\n    X_test_mesh = np.c_[xx.ravel(), yy.ravel()]\n    \n    # We need to create a complete feature vector for prediction\n    X_test_full = np.zeros((X_test_mesh.shape[0], X.shape[1]))\n    X_test_full[:, feature_idx[0]] = X_test_mesh[:, 0]\n    X_test_full[:, feature_idx[1]] = X_test_mesh[:, 1]\n    \n    # Predict the class for each point in the mesh\n    Z = model.predict(X_test_full)\n    Z = Z.reshape(xx.shape)\n    \n    # Plot the decision boundary\n    plt.figure(figsize=(10, 8))\n    plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.RdYlBu)\n    \n    # Plot the training points\n    scatter = plt.scatter(X_vis[:, 0], X_vis[:, 1], c=y, edgecolors='k', cmap=plt.cm.RdYlBu)\n    plt.xlabel(feature_names[feature_idx[0]])\n    plt.ylabel(feature_names[feature_idx[1]])\n    plt.title(f'KNN Decision Boundaries (K={optimal_k})')\n    plt.legend(handles=scatter.legend_elements()[0], labels=target_names)\n    plt.show()\n\n# Train a KNN model on all data for visualization\nknn_vis = KNeighborsClassifier(n_neighbors=optimal_k)\nknn_vis.fit(X_scaled, y)\n\n# Visualize the decision boundaries using petal features\nplot_decision_boundaries(X_scaled, y, knn_vis, feature_idx=(2, 3))\n\n# Classify a new iris flower\nnew_flower = np.array([[5.8, 2.7, 4.1, 1.0]])  # Values from our example\nnew_flower_scaled = scaler.transform(new_flower)\nprediction = knn.predict(new_flower_scaled)\nprobabilities = knn.predict_proba(new_flower_scaled)\n\nprint(f\"\\nNew Flower Measurements:\")\nfor i, name in enumerate(feature_names):\n    print(f\"{name}: {new_flower[0][i]} cm\")\n\nprint(f\"\\nPredicted Species: {target_names[prediction[0]]}\")\nprint(f\"Prediction Probabilities:\")\nfor i, species in enumerate(target_names):\n    print(f\"{species}: {probabilities[0][i]:.4f}\")\n\n# Find and display the K nearest neighbors to the new flower\ndistances, indices = knn.kneighbors(new_flower_scaled)\n\nprint(f\"\\n{optimal_k} Nearest Neighbors:\")\nfor i in range(optimal_k):\n    neighbor_idx = indices[0][i]\n    distance = distances[0][i]\n    neighbor = X[neighbor_idx]\n    species = target_names[y[neighbor_idx]]\n    print(f\"Neighbor {i+1}: {species} - Distance: {distance:.4f}\")\n    for j, name in enumerate(feature_names):\n        print(f\"  {name}: {neighbor[j]} cm\")\n"})}),"\n",(0,t.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,t.jsx)(n.p,{children:"\u2705 Built a K-Nearest Neighbors model for Iris flower classification"}),"\n",(0,t.jsx)(n.p,{children:"\u2705 Explored how to find the optimal K value using cross-validation"}),"\n",(0,t.jsx)(n.p,{children:"\u2705 Visualized decision boundaries to understand how KNN classifies data"}),"\n",(0,t.jsx)(n.p,{children:"\u2705 Evaluated model performance using accuracy and confusion matrix"}),"\n",(0,t.jsx)(n.p,{children:"\u2705 Demonstrated how to identify the nearest neighbors for a new data point"})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>r,x:()=>l});var i=s(6540);const t={},a=i.createContext(t);function r(e){const n=i.useContext(a);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),i.createElement(a.Provider,{value:n},e.children)}}}]);