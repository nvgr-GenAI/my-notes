"use strict";(self.webpackChunkmy_notes=self.webpackChunkmy_notes||[]).push([[8934],{1593:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>o,contentTitle:()=>l,default:()=>p,frontMatter:()=>a,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"ml/supervised/decision-tree-regression","title":"Decision Tree Regression","description":"Understanding Decision Tree algorithms for regression tasks","source":"@site/docs/ml/supervised/decision-tree-regression.md","sourceDirName":"ml/supervised","slug":"/ml/supervised/decision-tree-regression","permalink":"/my-notes/docs/ml/supervised/decision-tree-regression","draft":false,"unlisted":false,"editUrl":"https://github.com/nvgr-GenAI/my-notes/edit/main/docs/ml/supervised/decision-tree-regression.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"title":"Decision Tree Regression","sidebar_position":4,"description":"Understanding Decision Tree algorithms for regression tasks"},"sidebar":"mlSidebar","previous":{"title":"Lasso Regression","permalink":"/my-notes/docs/ml/supervised/lasso-regression"},"next":{"title":"Random Forest Regression","permalink":"/my-notes/docs/ml/supervised/random-forest-regression"}}');var t=r(4848),s=r(8453);const a={title:"Decision Tree Regression",sidebar_position:4,description:"Understanding Decision Tree algorithms for regression tasks"},l="Decision Tree Regression",o={},d=[{value:"1. How Regression Trees Work",id:"1-how-regression-trees-work",level:2},{value:"Splitting Criteria:",id:"splitting-criteria",level:3},{value:"Stopping Criteria:",id:"stopping-criteria",level:3},{value:"2. Example Use Case: House Price Prediction",id:"2-example-use-case-house-price-prediction",level:2},{value:"Scenario",id:"scenario",level:3},{value:"Dataset Sample",id:"dataset-sample",level:3},{value:"Decision Tree Approach",id:"decision-tree-approach",level:3},{value:"3. Advantages of Decision Trees for Regression",id:"3-advantages-of-decision-trees-for-regression",level:2},{value:"4. Limitations",id:"4-limitations",level:2},{value:"5. Real-World Applications",id:"5-real-world-applications",level:2},{value:"6. Implementation Example",id:"6-implementation-example",level:2},{value:"Summary",id:"summary",level:2},{value:"Comparing Regression Trees to Classification Trees",id:"comparing-regression-trees-to-classification-trees",level:2}];function c(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"decision-tree-regression",children:"Decision Tree Regression"})}),"\n",(0,t.jsxs)(n.p,{children:["Decision Trees for regression are ",(0,t.jsx)(n.strong,{children:"supervised learning algorithms"})," used to predict continuous target variables. Unlike classification trees that predict categories, regression trees predict numerical values by splitting the feature space and approximating the target with the mean value in each region."]}),"\n",(0,t.jsx)(n.h2,{id:"1-how-regression-trees-work",children:"1. How Regression Trees Work"}),"\n",(0,t.jsxs)(n.p,{children:["Decision trees use a ",(0,t.jsx)(n.strong,{children:"hierarchical, tree-like structure"})," where:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Root Node"}),": Starting point, represents entire dataset"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Decision Nodes"}),": Points where data is split based on feature values"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Leaf Nodes"}),": Terminal nodes that provide the numerical output (typically the mean of target values in that region)"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"The algorithm works by:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Selecting the best feature"})," to split the data at each node"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Creating child nodes"})," based on the split"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Repeating recursively"})," until stopping criteria are met"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"splitting-criteria",children:"Splitting Criteria:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Mean Squared Error (MSE)"}),": The most common criterion, minimizes the average of squared differences between target and predicted values"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Mean Absolute Error (MAE)"}),": Minimizes the average of absolute differences"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Friedman's MSE"}),": A modified version of MSE used in some implementations"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"stopping-criteria",children:"Stopping Criteria:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Maximum depth reached"}),"\n",(0,t.jsx)(n.li,{children:"Minimum samples required for a split"}),"\n",(0,t.jsx)(n.li,{children:"Minimum samples required at a leaf node"}),"\n",(0,t.jsx)(n.li,{children:"Maximum leaf nodes reached"}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"2-example-use-case-house-price-prediction",children:"2. Example Use Case: House Price Prediction"}),"\n",(0,t.jsx)(n.h3,{id:"scenario",children:"Scenario"}),"\n",(0,t.jsx)(n.p,{children:"A real estate company wants to predict house prices based on property characteristics."}),"\n",(0,t.jsx)(n.h3,{id:"dataset-sample",children:"Dataset Sample"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Size (sqft)"}),(0,t.jsx)(n.th,{children:"Bedrooms"}),(0,t.jsx)(n.th,{children:"Age (years)"}),(0,t.jsx)(n.th,{children:"Location Rating"}),(0,t.jsx)(n.th,{children:"Price ($)"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"1200"}),(0,t.jsx)(n.td,{children:"2"}),(0,t.jsx)(n.td,{children:"15"}),(0,t.jsx)(n.td,{children:"7"}),(0,t.jsx)(n.td,{children:"250,000"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"2400"}),(0,t.jsx)(n.td,{children:"4"}),(0,t.jsx)(n.td,{children:"5"}),(0,t.jsx)(n.td,{children:"9"}),(0,t.jsx)(n.td,{children:"550,000"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"1500"}),(0,t.jsx)(n.td,{children:"3"}),(0,t.jsx)(n.td,{children:"10"}),(0,t.jsx)(n.td,{children:"6"}),(0,t.jsx)(n.td,{children:"320,000"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"1800"}),(0,t.jsx)(n.td,{children:"3"}),(0,t.jsx)(n.td,{children:"20"}),(0,t.jsx)(n.td,{children:"8"}),(0,t.jsx)(n.td,{children:"380,000"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"3000"}),(0,t.jsx)(n.td,{children:"5"}),(0,t.jsx)(n.td,{children:"2"}),(0,t.jsx)(n.td,{children:"10"}),(0,t.jsx)(n.td,{children:"750,000"})]})]})]}),"\n",(0,t.jsx)(n.h3,{id:"decision-tree-approach",children:"Decision Tree Approach"}),"\n",(0,t.jsx)(n.p,{children:"The algorithm will:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Select the most informative feature for the first split (e.g., Size)"}),"\n",(0,t.jsx)(n.li,{children:"Split the data based on this feature"}),"\n",(0,t.jsx)(n.li,{children:"Continue splitting recursively on each subset"}),"\n",(0,t.jsx)(n.li,{children:"Form a tree structure that can predict house prices for new properties"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"For example, the resulting tree might look like:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"Size \u2264 1500 sqft?\n\u251c\u2500\u2500 Yes \u2192 Location Rating \u2264 7?\n\u2502   \u251c\u2500\u2500 Yes \u2192 $230,000\n\u2502   \u2514\u2500\u2500 No \u2192 $290,000\n\u2514\u2500\u2500 No \u2192 Bedrooms \u2264 3?\n    \u251c\u2500\u2500 Yes \u2192 Age \u2264 10?\n    \u2502   \u251c\u2500\u2500 Yes \u2192 $400,000\n    \u2502   \u2514\u2500\u2500 No \u2192 $350,000\n    \u2514\u2500\u2500 No \u2192 $650,000\n"})}),"\n",(0,t.jsx)(n.p,{children:"For a new property with:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Size: 1800 sqft"}),"\n",(0,t.jsx)(n.li,{children:"Bedrooms: 3"}),"\n",(0,t.jsx)(n.li,{children:"Age: 7 years"}),"\n",(0,t.jsx)(n.li,{children:"Location Rating: 8"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"We would trace the path: Size > 1500 sqft \u2192 Bedrooms \u2264 3 \u2192 Age \u2264 10 \u2192 $400,000"}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"3-advantages-of-decision-trees-for-regression",children:"3. Advantages of Decision Trees for Regression"}),"\n",(0,t.jsx)(n.p,{children:"\u2705 Simple to understand and visualize decision boundaries"}),"\n",(0,t.jsx)(n.p,{children:"\u2705 Requires little data preprocessing (no normalization needed)"}),"\n",(0,t.jsx)(n.p,{children:"\u2705 Can handle both numerical and categorical data"}),"\n",(0,t.jsx)(n.p,{children:"\u2705 Implicitly performs feature selection"}),"\n",(0,t.jsx)(n.p,{children:"\u2705 Non-parametric (no assumptions about data distribution)"}),"\n",(0,t.jsx)(n.p,{children:"\u2705 Handles non-linear relationships well"}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"4-limitations",children:"4. Limitations"}),"\n",(0,t.jsx)(n.p,{children:"\u274c Prone to overfitting, especially with deep trees"}),"\n",(0,t.jsx)(n.p,{children:"\u274c Poor at extrapolation (predictions outside the range of training data)"}),"\n",(0,t.jsx)(n.p,{children:"\u274c Can be unstable (small variations in data can produce very different trees)"}),"\n",(0,t.jsx)(n.p,{children:"\u274c May struggle with smooth, continuous functions"}),"\n",(0,t.jsxs)(n.p,{children:["Techniques like ",(0,t.jsx)(n.strong,{children:"pruning"}),", ",(0,t.jsx)(n.strong,{children:"setting maximum depth"}),", and ",(0,t.jsx)(n.strong,{children:"ensemble methods"})," (Random Forests) can address some of these limitations."]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"5-real-world-applications",children:"5. Real-World Applications"}),"\n",(0,t.jsxs)(n.p,{children:["\ud83c\udfe0 ",(0,t.jsx)(n.strong,{children:"Real Estate Valuation"})," \u2013 Predicting property prices"]}),"\n",(0,t.jsxs)(n.p,{children:["\ud83d\udcc8 ",(0,t.jsx)(n.strong,{children:"Financial Forecasting"})," \u2013 Predicting stock prices or economic indicators"]}),"\n",(0,t.jsxs)(n.p,{children:["\u26c5 ",(0,t.jsx)(n.strong,{children:"Weather Prediction"})," \u2013 Forecasting temperature, rainfall, etc."]}),"\n",(0,t.jsxs)(n.p,{children:["\u26a1 ",(0,t.jsx)(n.strong,{children:"Energy Consumption"})," \u2013 Estimating household or industrial energy usage"]}),"\n",(0,t.jsxs)(n.p,{children:["\ud83d\ude97 ",(0,t.jsx)(n.strong,{children:"Vehicle Pricing"})," \u2013 Determining used car values"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"6-implementation-example",children:"6. Implementation Example"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# Install Dependencies\n# pip install numpy pandas scikit-learn matplotlib seaborn \n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.tree import DecisionTreeRegressor, plot_tree, export_graphviz\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\nfrom sklearn.preprocessing import LabelEncoder\nimport math\n\n# Sample house price dataset\ndata = {\n    'Size': [1200, 2400, 1500, 1800, 3000, 1650, 2100, 2700, 1400, 1950, \n              2200, 1300, 2800, 3200, 1750, 2500, 1600, 2300, 3500, 2000],\n    'Bedrooms': [2, 4, 3, 3, 5, 3, 4, 4, 2, 3, \n                 4, 2, 5, 5, 3, 4, 3, 4, 6, 3],\n    'Age': [15, 5, 10, 20, 2, 12, 8, 3, 25, 15, \n            10, 30, 4, 1, 18, 7, 9, 6, 2, 11],\n    'LocationRating': [7, 9, 6, 8, 10, 7, 8, 9, 5, 7, \n                       8, 6, 9, 10, 7, 8, 6, 8, 10, 7],\n    'Price': [250000, 550000, 320000, 380000, 750000, 340000, 470000, 620000, 230000, 400000,\n              480000, 210000, 670000, 820000, 360000, 580000, 330000, 510000, 900000, 420000]\n}\n\n# Convert to DataFrame\ndf = pd.DataFrame(data)\n\n# Display the first few rows\nprint(df.head())\n\n# Basic data exploration\nprint(\"\\nData Summary:\")\nprint(df.describe())\n\n# Visualize the data\nplt.figure(figsize=(14, 10))\n\nplt.subplot(2, 2, 1)\nplt.scatter(df['Size'], df['Price'])\nplt.title('Size vs. Price')\nplt.xlabel('Size (sqft)')\nplt.ylabel('Price ($)')\n\nplt.subplot(2, 2, 2)\nplt.scatter(df['Bedrooms'], df['Price'])\nplt.title('Bedrooms vs. Price')\nplt.xlabel('Number of Bedrooms')\nplt.ylabel('Price ($)')\n\nplt.subplot(2, 2, 3)\nplt.scatter(df['Age'], df['Price'])\nplt.title('Age vs. Price')\nplt.xlabel('Age (years)')\nplt.ylabel('Price ($)')\n\nplt.subplot(2, 2, 4)\nplt.scatter(df['LocationRating'], df['Price'])\nplt.title('Location Rating vs. Price')\nplt.xlabel('Location Rating')\nplt.ylabel('Price ($)')\n\nplt.tight_layout()\nplt.show()\n\n# Correlation matrix\nplt.figure(figsize=(10, 8))\ncorrelation_matrix = df.corr()\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\nplt.title('Correlation Matrix')\nplt.show()\n\n# Prepare data for the model\nX = df[['Size', 'Bedrooms', 'Age', 'LocationRating']]\ny = df['Price']\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n\n# Cross-validation to find optimal max_depth\ndepth_range = range(1, 10)\nmse_scores = []\n\nfor depth in depth_range:\n    dt_regressor = DecisionTreeRegressor(max_depth=depth, random_state=42)\n    # Use negative MSE as scoring since cross_val_score maximizes the score\n    scores = cross_val_score(dt_regressor, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n    mse_scores.append(-scores.mean())  # Convert back to positive MSE\n\n# Plot cross-validation results\nplt.figure(figsize=(10, 6))\nplt.plot(depth_range, mse_scores, marker='o')\nplt.xlabel('Maximum Depth')\nplt.ylabel('Mean Squared Error')\nplt.title('Finding Optimal Tree Depth')\nplt.grid(True)\nplt.show()\n\n# Convert MSE to RMSE for easier interpretation\nrmse_scores = [math.sqrt(mse) for mse in mse_scores]\nplt.figure(figsize=(10, 6))\nplt.plot(depth_range, rmse_scores, marker='o')\nplt.xlabel('Maximum Depth')\nplt.ylabel('Root Mean Squared Error')\nplt.title('Finding Optimal Tree Depth (RMSE)')\nplt.grid(True)\nplt.show()\n\n# Get the best depth\nbest_depth = depth_range[np.argmin(mse_scores)]\nprint(f\"\\nOptimal tree depth: {best_depth}\")\n\n# Train the final model with the best depth\ndt_regressor = DecisionTreeRegressor(max_depth=best_depth, random_state=42)\ndt_regressor.fit(X_train, y_train)\n\n# Make predictions\ny_pred = dt_regressor.predict(X_test)\n\n# Model evaluation\nmse = mean_squared_error(y_test, y_pred)\nrmse = math.sqrt(mse)\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(f\"Mean Squared Error: {mse:.2f}\")\nprint(f\"Root Mean Squared Error: {rmse:.2f}\")\nprint(f\"Mean Absolute Error: {mae:.2f}\")\nprint(f\"R\xb2 Score: {r2:.4f}\")\n\n# Visualize predictions vs actual values\nplt.figure(figsize=(10, 6))\nplt.scatter(y_test, y_pred)\nplt.xlabel('Actual Price')\nplt.ylabel('Predicted Price')\nplt.title('Actual vs Predicted Prices')\n\n# Add perfect prediction line\nmin_val = min(min(y_test), min(y_pred))\nmax_val = max(max(y_test), max(y_pred))\nplt.plot([min_val, max_val], [min_val, max_val], 'r--')\nplt.show()\n\n# Visualize residuals\nresiduals = y_test - y_pred\nplt.figure(figsize=(10, 6))\nplt.scatter(y_pred, residuals)\nplt.axhline(y=0, color='r', linestyle='-')\nplt.xlabel('Predicted Price')\nplt.ylabel('Residual')\nplt.title('Residual Plot')\nplt.show()\n\n# Visualize the decision tree\nplt.figure(figsize=(20, 10))\nfeature_names = ['Size (sqft)', 'Bedrooms', 'Age (years)', 'Location Rating']\nplot_tree(dt_regressor, filled=True, feature_names=feature_names, rounded=True)\nplt.title(f\"Decision Tree Regressor (Max Depth = {best_depth})\")\nplt.show()\n\n# Feature importance\nimportances = dt_regressor.feature_importances_\nindices = np.argsort(importances)[::-1]\n\nplt.figure(figsize=(10, 6))\nplt.title('Feature Importances')\nplt.bar(range(X.shape[1]), importances[indices], align='center')\nplt.xticks(range(X.shape[1]), np.array(feature_names)[indices])\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nFeature Importances:\")\nfor i, feature in enumerate(np.array(feature_names)[indices]):\n    print(f\"{feature}: {importances[indices[i]]:.4f}\")\n\n# Predict price for a new house\nnew_house = np.array([[1800, 3, 7, 8]])\nprediction = dt_regressor.predict(new_house)\n\nprint(\"\\nNew House:\")\nprint(f\"Size: 1800 sqft\")\nprint(f\"Bedrooms: 3\")\nprint(f\"Age: 7 years\")\nprint(f\"Location Rating: 8\")\nprint(f\"\\nPredicted Price: ${prediction[0]:,.2f}\")\n\n# Find the path through the decision tree for the new house\ndef get_decision_path(tree, X, feature_names):\n    node_indicator = tree.decision_path(X)\n    leaf_id = tree.apply(X)\n    \n    feature = tree.tree_.feature\n    threshold = tree.tree_.threshold\n    value = tree.tree_.value\n    \n    node_index = node_indicator.indices[node_indicator.indptr[0]:node_indicator.indptr[1]]\n    \n    rules = []\n    for node_id in node_index:\n        # Continue to the next node if not a leaf\n        if leaf_id[0] != node_id:\n            # If the feature value is below the threshold, follow the left path\n            if X[0, feature[node_id]] <= threshold[node_id]:\n                threshold_sign = \"<=\"\n            else:\n                threshold_sign = \">\"\n                \n            rules.append(f\"{feature_names[feature[node_id]]} {threshold_sign} {threshold[node_id]}\")\n        else:\n            # If it's a leaf node, get the prediction value\n            rules.append(f\"Predicted Price: ${value[node_id][0][0]:,.2f}\")\n    \n    return rules\n\ndecision_path = get_decision_path(dt_regressor, new_house, feature_names)\nprint(\"\\nDecision Path:\")\nfor i, rule in enumerate(decision_path):\n    print(f\"{i+1}. {rule}\")\n"})}),"\n",(0,t.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,t.jsx)(n.p,{children:"\u2705 Built a Decision Tree regression model for house price prediction"}),"\n",(0,t.jsx)(n.p,{children:"\u2705 Used cross-validation to find the optimal tree depth"}),"\n",(0,t.jsx)(n.p,{children:"\u2705 Visualized the decision tree for interpretability"}),"\n",(0,t.jsx)(n.p,{children:"\u2705 Analyzed feature importance to understand key factors"}),"\n",(0,t.jsx)(n.p,{children:"\u2705 Demonstrated making and explaining predictions for new houses with exact values"}),"\n",(0,t.jsx)(n.p,{children:"\u2705 Evaluated model using RMSE, MAE, and R\xb2 metrics"}),"\n",(0,t.jsx)(n.h2,{id:"comparing-regression-trees-to-classification-trees",children:"Comparing Regression Trees to Classification Trees"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Aspect"}),(0,t.jsx)(n.th,{children:"Classification Trees"}),(0,t.jsx)(n.th,{children:"Regression Trees"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Target Variable"})}),(0,t.jsx)(n.td,{children:"Categorical (classes)"}),(0,t.jsx)(n.td,{children:"Numerical (continuous)"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Leaf Nodes"})}),(0,t.jsx)(n.td,{children:"Store class probabilities"}),(0,t.jsx)(n.td,{children:"Store mean target values"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Splitting Criteria"})}),(0,t.jsx)(n.td,{children:"Gini impurity, Entropy"}),(0,t.jsx)(n.td,{children:"MSE, MAE"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Prediction"})}),(0,t.jsx)(n.td,{children:"Class label or probabilities"}),(0,t.jsx)(n.td,{children:"Numerical value"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Evaluation Metrics"})}),(0,t.jsx)(n.td,{children:"Accuracy, Precision, Recall, F1"}),(0,t.jsx)(n.td,{children:"RMSE, MAE, R\xb2"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Visualization"})}),(0,t.jsx)(n.td,{children:"Classes shown with colors"}),(0,t.jsx)(n.td,{children:"Values shown in leaves"})]})]})]})]})}function p(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>a,x:()=>l});var i=r(6540);const t={},s=i.createContext(t);function a(e){const n=i.useContext(s);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),i.createElement(s.Provider,{value:n},e.children)}}}]);