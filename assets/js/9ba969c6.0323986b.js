"use strict";(self.webpackChunkmy_notes=self.webpackChunkmy_notes||[]).push([[8080],{4814:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>o,contentTitle:()=>l,default:()=>p,frontMatter:()=>a,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"ml/supervised/neural-networks-classification","title":"Neural Networks for Classification","description":"Neural Networks for Classification are powerful supervised learning models that can learn complex patterns in data to categorize inputs into discrete classes. These networks excel at classification tasks ranging from simple binary decisions to complex multi-class problems.","source":"@site/docs/ml/supervised/neural-networks-classification.md","sourceDirName":"ml/supervised","slug":"/ml/supervised/neural-networks-classification","permalink":"/my-notes/docs/ml/supervised/neural-networks-classification","draft":false,"unlisted":false,"editUrl":"https://github.com/nvgr-GenAI/my-notes/edit/main/docs/ml/supervised/neural-networks-classification.md","tags":[],"version":"current","frontMatter":{},"sidebar":"mlSidebar","previous":{"title":"SVM Classification","permalink":"/my-notes/docs/ml/supervised/svm-classification"},"next":{"title":"Examples of Supervised Learning","permalink":"/my-notes/docs/ml/supervised/examples"}}');var t=i(4848),r=i(8453);const a={},l="Neural Networks for Classification",o={},c=[{value:"How Neural Networks Perform Classification",id:"how-neural-networks-perform-classification",level:2},{value:"Neural Network Architecture for Classification",id:"neural-network-architecture-for-classification",level:2},{value:"Key Components for Classification",id:"key-components-for-classification",level:2},{value:"Implementation: Binary Classification Example",id:"implementation-binary-classification-example",level:2},{value:"Implementation: Multi-class Classification Example",id:"implementation-multi-class-classification-example",level:2},{value:"Handling Imbalanced Datasets",id:"handling-imbalanced-datasets",level:2},{value:"Hyperparameter Tuning for Classification",id:"hyperparameter-tuning-for-classification",level:2},{value:"Common Applications of Neural Networks for Classification",id:"common-applications-of-neural-networks-for-classification",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...n.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.header,{children:(0,t.jsx)(e.h1,{id:"neural-networks-for-classification",children:"Neural Networks for Classification"})}),"\n",(0,t.jsx)(e.p,{children:"Neural Networks for Classification are powerful supervised learning models that can learn complex patterns in data to categorize inputs into discrete classes. These networks excel at classification tasks ranging from simple binary decisions to complex multi-class problems."}),"\n",(0,t.jsx)(e.h2,{id:"how-neural-networks-perform-classification",children:"How Neural Networks Perform Classification"}),"\n",(0,t.jsx)(e.p,{children:"Neural networks for classification typically:"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Process input features"})," through multiple hidden layers"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Transform data"})," using weights, biases, and activation functions"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Output class probabilities"})," using a specialized output layer:","\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Binary classification"}),": Single output neuron with sigmoid activation (0-1 range)"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Multi-class classification"}),": Multiple output neurons with softmax activation (probabilities sum to 1)"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"neural-network-architecture-for-classification",children:"Neural Network Architecture for Classification"}),"\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.img,{src:"https://i.imgur.com/JMfFnQE.png",alt:"Neural Network Classification Architecture"})}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Input Layer"}),": One neuron per feature in your dataset"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Hidden Layers"}),": Multiple layers with varying numbers of neurons to learn complex patterns"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Output Layer"}),":","\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"For binary classification: 1 neuron with sigmoid activation"}),"\n",(0,t.jsx)(e.li,{children:"For multi-class classification: N neurons (one per class) with softmax activation"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"key-components-for-classification",children:"Key Components for Classification"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Loss Functions"}),":"]}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Binary classification: Binary cross-entropy"}),"\n",(0,t.jsx)(e.li,{children:"Multi-class classification: Categorical cross-entropy"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Evaluation Metrics"}),":"]}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Accuracy"}),"\n",(0,t.jsx)(e.li,{children:"Precision and Recall"}),"\n",(0,t.jsx)(e.li,{children:"F1-Score"}),"\n",(0,t.jsx)(e.li,{children:"ROC AUC"}),"\n",(0,t.jsx)(e.li,{children:"Confusion Matrix"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Output Layer Design"}),":"]}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:["Binary: ",(0,t.jsx)(e.code,{children:"Dense(1, activation='sigmoid')"})]}),"\n",(0,t.jsxs)(e.li,{children:["Multi-class: ",(0,t.jsx)(e.code,{children:"Dense(num_classes, activation='softmax')"})]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"implementation-binary-classification-example",children:"Implementation: Binary Classification Example"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"import numpy as np\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam\nimport matplotlib.pyplot as plt\n\n# Generate a sample binary classification dataset\nX, y = make_classification(n_samples=1000, n_features=20, n_informative=15, \n                           n_redundant=5, random_state=42)\n\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Scale the features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Create a neural network for binary classification\nmodel = Sequential([\n    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n    Dropout(0.2),\n    Dense(32, activation='relu'),\n    Dropout(0.2),\n    Dense(16, activation='relu'),\n    Dense(1, activation='sigmoid')  # Binary classification output\n])\n\n# Compile the model\nmodel.compile(optimizer=Adam(learning_rate=0.001), \n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\n# Display model architecture\nmodel.summary()\n\n# Train the model\nhistory = model.fit(X_train_scaled, y_train, \n                    epochs=50, \n                    batch_size=32, \n                    validation_split=0.2, \n                    verbose=1)\n\n# Plot training history\nplt.figure(figsize=(12, 5))\nplt.subplot(1, 2, 1)\nplt.plot(history.history['accuracy'], label='Training Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(history.history['loss'], label='Training Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n# Evaluate the model\ny_pred_proba = model.predict(X_test_scaled)\ny_pred = (y_pred_proba > 0.5).astype(int).flatten()\n\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Test Accuracy: {accuracy:.4f}\")\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred))\nprint(\"\\nConfusion Matrix:\")\nprint(confusion_matrix(y_test, y_pred))\n\n# Plot ROC curve\nfrom sklearn.metrics import roc_curve, auc\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\nroc_auc = auc(fpr, tpr)\n\nplt.figure(figsize=(8, 6))\nplt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\nplt.plot([0, 1], [0, 1], color='gray', lw=1, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC)')\nplt.legend(loc=\"lower right\")\nplt.show()\n"})}),"\n",(0,t.jsx)(e.h2,{id:"implementation-multi-class-classification-example",children:"Implementation: Multi-class Classification Example"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"from sklearn.datasets import load_iris\nfrom tensorflow.keras.utils import to_categorical\n\n# Load Iris dataset for multi-class classification\niris = load_iris()\nX, y = iris.data, iris.target\n\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Scale the features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Convert labels to one-hot encoding\ny_train_onehot = to_categorical(y_train)\ny_test_onehot = to_categorical(y_test)\n\n# Create a neural network for multi-class classification\nmodel_multiclass = Sequential([\n    Dense(16, activation='relu', input_shape=(X_train.shape[1],)),\n    Dense(8, activation='relu'),\n    Dense(3, activation='softmax')  # Multi-class classification output (3 classes)\n])\n\n# Compile the model\nmodel_multiclass.compile(optimizer=Adam(learning_rate=0.001), \n                         loss='categorical_crossentropy',\n                         metrics=['accuracy'])\n\n# Display model architecture\nmodel_multiclass.summary()\n\n# Train the model\nhistory = model_multiclass.fit(X_train_scaled, y_train_onehot, \n                               epochs=50, \n                               batch_size=16, \n                               validation_split=0.2, \n                               verbose=1)\n\n# Evaluate the model\ny_pred_proba = model_multiclass.predict(X_test_scaled)\ny_pred = np.argmax(y_pred_proba, axis=1)\n\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Test Accuracy: {accuracy:.4f}\")\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred, target_names=iris.target_names))\nprint(\"\\nConfusion Matrix:\")\nprint(confusion_matrix(y_test, y_pred))\n"})}),"\n",(0,t.jsx)(e.h2,{id:"handling-imbalanced-datasets",children:"Handling Imbalanced Datasets"}),"\n",(0,t.jsx)(e.p,{children:"Neural networks can struggle with imbalanced datasets. Common strategies include:"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Class weights"}),": Assign higher weights to minority classes"]}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"# Calculate class weights\nfrom sklearn.utils.class_weight import compute_class_weight\nclass_weights = compute_class_weight('balanced', np.unique(y_train), y_train)\nclass_weight_dict = dict(enumerate(class_weights))\n\n# Use in model training\nmodel.fit(X_train, y_train, class_weight=class_weight_dict, ...)\n"})}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Resampling"}),": Either undersample the majority class or oversample the minority class"]}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"from imblearn.over_sampling import SMOTE\nsmote = SMOTE(random_state=42)\nX_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n"})}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Different metrics"}),": Focus on precision, recall, F1-score, or AUC instead of accuracy"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"hyperparameter-tuning-for-classification",children:"Hyperparameter Tuning for Classification"}),"\n",(0,t.jsx)(e.p,{children:"Key hyperparameters to tune:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Network architecture"}),": Number of layers and neurons"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Learning rate"}),": Typically between 0.1 and 0.0001"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Batch size"}),": Common values: 16, 32, 64, 128"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Activation functions"}),": ReLU, LeakyReLU, ELU"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Regularization"}),": L1, L2, or both (ElasticNet)"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Dropout rate"}),": Typically between 0.2 and 0.5"]}),"\n"]}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"# Example of hyperparameter grid search\nfrom tensorflow.keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import GridSearchCV\n\ndef create_model(learning_rate=0.001, neurons=64, dropout_rate=0.2):\n    model = Sequential([\n        Dense(neurons, activation='relu', input_shape=(X_train.shape[1],)),\n        Dropout(dropout_rate),\n        Dense(neurons // 2, activation='relu'),\n        Dropout(dropout_rate),\n        Dense(1, activation='sigmoid')\n    ])\n    model.compile(optimizer=Adam(learning_rate=learning_rate), \n                 loss='binary_crossentropy',\n                 metrics=['accuracy'])\n    return model\n\n# Create model\nkeras_classifier = KerasClassifier(build_fn=create_model, verbose=0)\n\n# Define parameter grid\nparam_grid = {\n    'learning_rate': [0.001, 0.01],\n    'epochs': [50, 100],\n    'batch_size': [16, 32, 64],\n    'neurons': [32, 64, 128],\n    'dropout_rate': [0.2, 0.3, 0.4]\n}\n\n# Grid search\ngrid = GridSearchCV(estimator=keras_classifier, param_grid=param_grid, cv=3)\ngrid_result = grid.fit(X_train_scaled, y_train)\n\n# Print results\nprint(f\"Best: {grid_result.best_score_:.4f} using {grid_result.best_params_}\")\n"})}),"\n",(0,t.jsx)(e.h2,{id:"common-applications-of-neural-networks-for-classification",children:"Common Applications of Neural Networks for Classification"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Image classification"}),": Identify objects in images"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Text classification"}),": Sentiment analysis, spam filtering"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Medical diagnosis"}),": Disease classification from symptoms or images"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Customer churn prediction"}),": Identify customers likely to leave"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Credit scoring"}),": Approve/deny loan applications"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Fraud detection"}),": Identify fraudulent transactions"]}),"\n"]})]})}function p(n={}){const{wrapper:e}={...(0,r.R)(),...n.components};return e?(0,t.jsx)(e,{...n,children:(0,t.jsx)(d,{...n})}):d(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>a,x:()=>l});var s=i(6540);const t={},r=s.createContext(t);function a(n){const e=s.useContext(r);return s.useMemo((function(){return"function"==typeof n?n(e):{...e,...n}}),[e,n])}function l(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(t):n.components||t:a(n.components),s.createElement(r.Provider,{value:e},n.children)}}}]);