"use strict";(self.webpackChunkmy_notes=self.webpackChunkmy_notes||[]).push([[5834],{1836:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>o,contentTitle:()=>l,default:()=>p,frontMatter:()=>r,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"ml/supervised/naive-bayes","title":"Naive Bayes","description":"Understanding Naive Bayes algorithms for probabilistic classification","source":"@site/docs/ml/supervised/naive-bayes.md","sourceDirName":"ml/supervised","slug":"/ml/supervised/naive-bayes","permalink":"/my-notes/docs/ml/supervised/naive-bayes","draft":false,"unlisted":false,"editUrl":"https://github.com/nvgr-GenAI/my-notes/edit/main/docs/ml/supervised/naive-bayes.md","tags":[],"version":"current","sidebarPosition":7,"frontMatter":{"title":"Naive Bayes","sidebar_position":7,"description":"Understanding Naive Bayes algorithms for probabilistic classification"},"sidebar":"mlSidebar","previous":{"title":"Random Forest Classification","permalink":"/my-notes/docs/ml/supervised/random-forest-classification"},"next":{"title":"K-Nearest Neighbors","permalink":"/my-notes/docs/ml/supervised/k-nearest-neighbors"}}');var a=i(4848),t=i(8453);const r={title:"Naive Bayes",sidebar_position:7,description:"Understanding Naive Bayes algorithms for probabilistic classification"},l="Naive Bayes",o={},d=[{value:"1. Types of Naive Bayes",id:"1-types-of-naive-bayes",level:2},{value:"A. Gaussian Naive Bayes",id:"a-gaussian-naive-bayes",level:3},{value:"B. Multinomial Naive Bayes",id:"b-multinomial-naive-bayes",level:3},{value:"C. Bernoulli Naive Bayes",id:"c-bernoulli-naive-bayes",level:3},{value:"2. How Naive Bayes Works",id:"2-how-naive-bayes-works",level:2},{value:"Classification Rule:",id:"classification-rule",level:3},{value:"3. Example Use Case: Email Spam Detection",id:"3-example-use-case-email-spam-detection",level:2},{value:"Scenario",id:"scenario",level:3},{value:"Dataset Sample",id:"dataset-sample",level:3},{value:"Naive Bayes Approach",id:"naive-bayes-approach",level:3},{value:"4. Advantages of Naive Bayes",id:"4-advantages-of-naive-bayes",level:2},{value:"5. Limitations",id:"5-limitations",level:2},{value:"6. Real-World Applications",id:"6-real-world-applications",level:2},{value:"7. Implementation Example",id:"7-implementation-example",level:2},{value:"Summary",id:"summary",level:2}];function c(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"naive-bayes",children:"Naive Bayes"})}),"\n",(0,a.jsxs)(n.p,{children:["Naive Bayes is a family of ",(0,a.jsx)(n.strong,{children:"probabilistic classifiers"})," based on applying Bayes' theorem with strong (naive) independence assumptions between the features. Despite its simplicity, Naive Bayes can be surprisingly effective, especially for text classification and when the independence assumption approximately holds."]}),"\n",(0,a.jsx)(n.h2,{id:"1-types-of-naive-bayes",children:"1. Types of Naive Bayes"}),"\n",(0,a.jsx)(n.h3,{id:"a-gaussian-naive-bayes",children:"A. Gaussian Naive Bayes"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Assumes features follow a normal distribution"}),"\n",(0,a.jsx)(n.li,{children:"Used for continuous data"}),"\n",(0,a.jsx)(n.li,{children:"Example: Medical diagnosis based on continuous measurements (blood pressure, temperature)"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"b-multinomial-naive-bayes",children:"B. Multinomial Naive Bayes"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Assumes features follow a multinomial distribution"}),"\n",(0,a.jsx)(n.li,{children:"Commonly used for document classification"}),"\n",(0,a.jsx)(n.li,{children:"Example: Email classification based on word frequencies"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"c-bernoulli-naive-bayes",children:"C. Bernoulli Naive Bayes"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Features are binary (0/1)"}),"\n",(0,a.jsx)(n.li,{children:"Used for text classification with binary word occurrence features"}),"\n",(0,a.jsx)(n.li,{children:"Example: Spam detection where features are presence/absence of specific words"}),"\n"]}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"2-how-naive-bayes-works",children:"2. How Naive Bayes Works"}),"\n",(0,a.jsxs)(n.p,{children:["Naive Bayes is based on ",(0,a.jsx)(n.strong,{children:"Bayes' theorem"}),":"]}),"\n",(0,a.jsx)(n.p,{children:"P(y|X) = [P(X|y) \xd7 P(y)] / P(X)"}),"\n",(0,a.jsx)(n.p,{children:"Where:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"P(y|X) is the posterior probability of class y given predictor X"}),"\n",(0,a.jsx)(n.li,{children:"P(X|y) is the likelihood of predictor X given class y"}),"\n",(0,a.jsx)(n.li,{children:"P(y) is the prior probability of class y"}),"\n",(0,a.jsx)(n.li,{children:"P(X) is the prior probability of predictor X"}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:'The "naive" assumption is that features are conditionally independent given the class:'}),"\n",(0,a.jsx)(n.p,{children:"P(X\u2081, X\u2082, ..., X\u2099|y) = P(X\u2081|y) \xd7 P(X\u2082|y) \xd7 ... \xd7 P(X\u2099|y)"}),"\n",(0,a.jsx)(n.p,{children:"This simplifies the calculation and makes the algorithm computationally efficient."}),"\n",(0,a.jsx)(n.h3,{id:"classification-rule",children:"Classification Rule:"}),"\n",(0,a.jsx)(n.p,{children:"Choose the class with the highest posterior probability:"}),"\n",(0,a.jsx)(n.p,{children:"y = argmax[y] P(y) \xd7 \u220f P(X\u1d62|y)"}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"3-example-use-case-email-spam-detection",children:"3. Example Use Case: Email Spam Detection"}),"\n",(0,a.jsx)(n.h3,{id:"scenario",children:"Scenario"}),"\n",(0,a.jsx)(n.p,{children:"An email service wants to filter spam emails based on the presence of certain words."}),"\n",(0,a.jsx)(n.h3,{id:"dataset-sample",children:"Dataset Sample"}),"\n",(0,a.jsxs)(n.table,{children:[(0,a.jsx)(n.thead,{children:(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.th,{children:"Email"}),(0,a.jsx)(n.th,{children:'Contains "Money"'}),(0,a.jsx)(n.th,{children:'Contains "Offer"'}),(0,a.jsx)(n.th,{children:'Contains "Meeting"'}),(0,a.jsx)(n.th,{children:'Contains "Report"'}),(0,a.jsx)(n.th,{children:"Is Spam"})]})}),(0,a.jsxs)(n.tbody,{children:[(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"Email 1"}),(0,a.jsx)(n.td,{children:"Yes"}),(0,a.jsx)(n.td,{children:"Yes"}),(0,a.jsx)(n.td,{children:"No"}),(0,a.jsx)(n.td,{children:"No"}),(0,a.jsx)(n.td,{children:"Yes"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"Email 2"}),(0,a.jsx)(n.td,{children:"No"}),(0,a.jsx)(n.td,{children:"No"}),(0,a.jsx)(n.td,{children:"Yes"}),(0,a.jsx)(n.td,{children:"Yes"}),(0,a.jsx)(n.td,{children:"No"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"Email 3"}),(0,a.jsx)(n.td,{children:"Yes"}),(0,a.jsx)(n.td,{children:"Yes"}),(0,a.jsx)(n.td,{children:"No"}),(0,a.jsx)(n.td,{children:"No"}),(0,a.jsx)(n.td,{children:"Yes"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"Email 4"}),(0,a.jsx)(n.td,{children:"No"}),(0,a.jsx)(n.td,{children:"No"}),(0,a.jsx)(n.td,{children:"No"}),(0,a.jsx)(n.td,{children:"Yes"}),(0,a.jsx)(n.td,{children:"No"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"Email 5"}),(0,a.jsx)(n.td,{children:"Yes"}),(0,a.jsx)(n.td,{children:"No"}),(0,a.jsx)(n.td,{children:"Yes"}),(0,a.jsx)(n.td,{children:"No"}),(0,a.jsx)(n.td,{children:"No"})]})]})]}),"\n",(0,a.jsx)(n.h3,{id:"naive-bayes-approach",children:"Naive Bayes Approach"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Calculate prior probabilities"}),": P(Spam) and P(Not Spam)"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Calculate likelihoods"}),": P(Word|Spam) and P(Word|Not Spam) for each word"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Apply Bayes' theorem"})," to get P(Spam|Words) and P(Not Spam|Words)"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Classify"})," based on which probability is higher"]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:'Let\'s calculate for a new email containing "Money" and "Offer":'}),"\n",(0,a.jsx)(n.p,{children:"From the training data:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"P(Spam) = 2/5 = 0.4"}),"\n",(0,a.jsx)(n.li,{children:"P(Not Spam) = 3/5 = 0.6"}),"\n",(0,a.jsx)(n.li,{children:'P("Money"|Spam) = 2/2 = 1.0'}),"\n",(0,a.jsx)(n.li,{children:'P("Money"|Not Spam) = 1/3 = 0.33'}),"\n",(0,a.jsx)(n.li,{children:'P("Offer"|Spam) = 2/2 = 1.0'}),"\n",(0,a.jsx)(n.li,{children:'P("Offer"|Not Spam) = 0/3 = 0.0'}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"Using Bayes' theorem (with Laplace smoothing to handle zero probabilities):"}),"\n",(0,a.jsx)(n.p,{children:'P(Spam|"Money", "Offer") \u221d P(Spam) \xd7 P("Money"|Spam) \xd7 P("Offer"|Spam) = 0.4 \xd7 1.0 \xd7 1.0 = 0.4'}),"\n",(0,a.jsx)(n.p,{children:'P(Not Spam|"Money", "Offer") \u221d P(Not Spam) \xd7 P("Money"|Not Spam) \xd7 P("Offer"|Not Spam) = 0.6 \xd7 0.33 \xd7 0.0 \u2248 0'}),"\n",(0,a.jsx)(n.p,{children:'Since P(Spam|"Money", "Offer") > P(Not Spam|"Money", "Offer"), the email is classified as spam.'}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"4-advantages-of-naive-bayes",children:"4. Advantages of Naive Bayes"}),"\n",(0,a.jsx)(n.p,{children:"\u2705 Simple, fast, and efficient"}),"\n",(0,a.jsx)(n.p,{children:"\u2705 Works well with high-dimensional data (like text)"}),"\n",(0,a.jsx)(n.p,{children:"\u2705 Requires less training data than many algorithms"}),"\n",(0,a.jsx)(n.p,{children:"\u2705 Not sensitive to irrelevant features"}),"\n",(0,a.jsx)(n.p,{children:"\u2705 Handles missing values by ignoring them during calculation"}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"5-limitations",children:"5. Limitations"}),"\n",(0,a.jsx)(n.p,{children:"\u274c Assumes strong independence between features (often not realistic)"}),"\n",(0,a.jsx)(n.p,{children:"\u274c Can be outperformed by more sophisticated models"}),"\n",(0,a.jsx)(n.p,{children:"\u274c Zero frequency problem (when a category has no occurrences of a feature)"}),"\n",(0,a.jsx)(n.p,{children:"\u274c Sensitive to how input data is prepared"}),"\n",(0,a.jsx)(n.p,{children:"\u274c Can't learn interactions between features"}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"6-real-world-applications",children:"6. Real-World Applications"}),"\n",(0,a.jsxs)(n.p,{children:["\ud83d\udce7 ",(0,a.jsx)(n.strong,{children:"Email Spam Filtering"})," \u2013 Identifying junk/spam emails"]}),"\n",(0,a.jsxs)(n.p,{children:["\ud83d\udcf0 ",(0,a.jsx)(n.strong,{children:"News Categorization"})," \u2013 Classifying articles by topic"]}),"\n",(0,a.jsxs)(n.p,{children:["\ud83c\udfad ",(0,a.jsx)(n.strong,{children:"Sentiment Analysis"})," \u2013 Determining positive/negative sentiment in text"]}),"\n",(0,a.jsxs)(n.p,{children:["\ud83d\udcc4 ",(0,a.jsx)(n.strong,{children:"Document Classification"})," \u2013 Organizing documents into categories"]}),"\n",(0,a.jsxs)(n.p,{children:["\ud83e\ude7a ",(0,a.jsx)(n.strong,{children:"Disease Diagnosis"})," \u2013 Classifying diseases based on symptoms"]}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"7-implementation-example",children:"7. Implementation Example"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"# Install Dependencies\n# pip install numpy pandas scikit-learn matplotlib seaborn\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom sklearn.pipeline import Pipeline\n\n# Sample email dataset\nemails = [\n    {'text': 'Get money fast with this exclusive offer free cash', 'label': 'spam'},\n    {'text': 'Meeting scheduled for tomorrow please bring your reports', 'label': 'not spam'},\n    {'text': 'Earn money with this amazing offer limited time', 'label': 'spam'},\n    {'text': 'Project report due next week team meeting tomorrow', 'label': 'not spam'},\n    {'text': 'Free money guaranteed offer click now to claim', 'label': 'spam'},\n    {'text': 'Reminder about performance review next week', 'label': 'not spam'},\n    {'text': 'Win cash prizes in our exclusive offer', 'label': 'spam'},\n    {'text': 'Notes from yesterday meeting check attachment', 'label': 'not spam'},\n    {'text': 'Double your money guaranteed investment offer', 'label': 'spam'},\n    {'text': 'Agenda for quarterly review meeting tomorrow', 'label': 'not spam'},\n    {'text': 'Free cash offer limited time only', 'label': 'spam'},\n    {'text': 'Please submit your expense reports by Friday', 'label': 'not spam'},\n    {'text': 'Financial freedom with this exclusive money making offer', 'label': 'spam'},\n    {'text': 'Team lunch scheduled for tomorrow noon', 'label': 'not spam'},\n    {'text': 'Urgent offer cash prizes claim now money', 'label': 'spam'},\n    {'text': 'New company policy document please review', 'label': 'not spam'},\n    {'text': 'Discount offer free trial money back guarantee', 'label': 'spam'},\n    {'text': 'Weekly meeting rescheduled to Thursday', 'label': 'not spam'},\n    {'text': 'Make money fast home business offer', 'label': 'spam'},\n    {'text': 'Quarterly report attached please review before meeting', 'label': 'not spam'}\n]\n\n# Convert to DataFrame\ndf = pd.DataFrame(emails)\n\n# Display the first few rows\nprint(df.head())\n\n# Create a binary label\ndf['is_spam'] = df['label'].apply(lambda x: 1 if x == 'spam' else 0)\n\n# Count words that appear in spam vs. not spam emails\ndef count_words_in_class(df, word, class_value):\n    mask = df['is_spam'] == class_value\n    texts_in_class = df.loc[mask, 'text'].str.lower()\n    count = sum(texts_in_class.str.contains(word.lower()))\n    return count, len(texts_in_class)\n\n# Calculate word frequencies for common spam/non-spam indicators\nwords = ['money', 'offer', 'meeting', 'report', 'free', 'cash', 'guarantee', 'review', 'team']\nword_stats = []\n\nfor word in words:\n    spam_count, total_spam = count_words_in_class(df, word, 1)\n    not_spam_count, total_not_spam = count_words_in_class(df, word, 0)\n    \n    spam_freq = spam_count / total_spam\n    not_spam_freq = not_spam_count / total_not_spam\n    \n    word_stats.append({\n        'word': word,\n        'spam_frequency': spam_freq,\n        'not_spam_frequency': not_spam_freq,\n        'frequency_ratio': spam_freq / not_spam_freq if not_spam_freq > 0 else float('inf')\n    })\n\nword_stats_df = pd.DataFrame(word_stats)\nprint(\"\\nWord frequencies in spam vs. non-spam emails:\")\nprint(word_stats_df.sort_values('frequency_ratio', ascending=False))\n\n# Visualize word frequencies\nplt.figure(figsize=(12, 6))\nword_stats_df = word_stats_df.sort_values('word')\nx = np.arange(len(word_stats_df))\nwidth = 0.35\n\nplt.bar(x - width/2, word_stats_df['spam_frequency'], width, label='Spam')\nplt.bar(x + width/2, word_stats_df['not_spam_frequency'], width, label='Not Spam')\n\nplt.xlabel('Words')\nplt.ylabel('Frequency')\nplt.title('Word Frequencies in Spam vs. Non-Spam Emails')\nplt.xticks(x, word_stats_df['word'], rotation=45)\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n# Split into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(df['text'], df['is_spam'], test_size=0.3, random_state=42)\n\n# Create a pipeline with CountVectorizer and Naive Bayes\npipeline = Pipeline([\n    ('vectorizer', CountVectorizer(stop_words='english')),  # Convert text to word count vectors\n    ('classifier', MultinomialNB())  # Multinomial Naive Bayes classifier\n])\n\n# Train the model\npipeline.fit(X_train, y_train)\n\n# Make predictions\ny_pred = pipeline.predict(X_test)\ny_prob = pipeline.predict_proba(X_test)[:, 1]  # Probability of being spam\n\n# Evaluate the model\nprint(\"\\nAccuracy:\", accuracy_score(y_test, y_pred))\n\nprint(\"\\nConfusion Matrix:\")\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Spam', 'Spam'], yticklabels=['Not Spam', 'Spam'])\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.show()\n\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred, target_names=['Not Spam', 'Spam']))\n\n# Extract feature names and their coefficients\nfeature_names = pipeline.named_steps['vectorizer'].get_feature_names_out()\ncoefficients = pipeline.named_steps['classifier'].feature_log_prob_\n\n# Words that strongly indicate spam (highest log-probability difference)\nlog_prob_diffs = coefficients[1] - coefficients[0]  # Spam - Not Spam\nstrongest_spam_indices = log_prob_diffs.argsort()[-10:][::-1]  # Top 10 spam indicators\nstrongest_ham_indices = log_prob_diffs.argsort()[:10]  # Top 10 ham indicators\n\nprint(\"\\nTop 10 words indicating spam:\")\nfor idx in strongest_spam_indices:\n    print(f\"{feature_names[idx]}: {np.exp(coefficients[1][idx]):.4f} vs {np.exp(coefficients[0][idx]):.4f}\")\n\nprint(\"\\nTop 10 words indicating not spam:\")\nfor idx in strongest_ham_indices:\n    print(f\"{feature_names[idx]}: {np.exp(coefficients[0][idx]):.4f} vs {np.exp(coefficients[1][idx]):.4f}\")\n\n# Test with new email examples\nnew_emails = [\n    \"Exclusive offer: Make money from home today!\",\n    \"Team meeting tomorrow to discuss quarterly reports\"\n]\n\nnew_predictions = pipeline.predict(new_emails)\nnew_probabilities = pipeline.predict_proba(new_emails)[:, 1]\n\nprint(\"\\nPredictions for new emails:\")\nfor i, email in enumerate(new_emails):\n    print(f\"Email: {email}\")\n    print(f\"Prediction: {'Spam' if new_predictions[i] == 1 else 'Not Spam'}\")\n    print(f\"Spam Probability: {new_probabilities[i]:.4f}\")\n    print()\n"})}),"\n",(0,a.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,a.jsx)(n.p,{children:"\u2705 Built a Naive Bayes model for email spam detection"}),"\n",(0,a.jsx)(n.p,{children:"\u2705 Understood how Bayes' theorem is applied to classify text"}),"\n",(0,a.jsx)(n.p,{children:"\u2705 Visualized word frequency differences between spam and non-spam emails"}),"\n",(0,a.jsx)(n.p,{children:"\u2705 Used a classification pipeline with text vectorization and Multinomial Naive Bayes"}),"\n",(0,a.jsx)(n.p,{children:"\u2705 Identified key words that indicate spam or legitimate emails"})]})}function p(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>l});var s=i(6540);const a={},t=s.createContext(a);function r(e){const n=s.useContext(t);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),s.createElement(t.Provider,{value:n},e.children)}}}]);