"use strict";(self.webpackChunkmy_notes=self.webpackChunkmy_notes||[]).push([[5577],{2828:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>o,contentTitle:()=>a,default:()=>h,frontMatter:()=>l,metadata:()=>r,toc:()=>d});const r=JSON.parse('{"id":"ml/supervised/svm-regression","title":"SVM Regression","description":"Support Vector Machine (SVM) Regression is a powerful supervised learning algorithm that performs regression by finding the optimal hyperplane that maximizes the margin while keeping the error within a specified threshold. Also known as SVR (Support Vector Regression), it\'s particularly effective for non-linear regression tasks.","source":"@site/docs/ml/supervised/svm-regression.md","sourceDirName":"ml/supervised","slug":"/ml/supervised/svm-regression","permalink":"/my-notes/docs/ml/supervised/svm-regression","draft":false,"unlisted":false,"editUrl":"https://github.com/nvgr-GenAI/my-notes/edit/main/docs/ml/supervised/svm-regression.md","tags":[],"version":"current","frontMatter":{},"sidebar":"mlSidebar","previous":{"title":"Random Forest Regression","permalink":"/my-notes/docs/ml/supervised/random-forest-regression"},"next":{"title":"Neural Networks for Regression","permalink":"/my-notes/docs/ml/supervised/neural-networks-regression"}}');var i=s(4848),t=s(8453);const l={},a="SVM Regression",o={},d=[{value:"How SVM Regression Works",id:"how-svm-regression-works",level:2},{value:"Types of SVM Regression",id:"types-of-svm-regression",level:2},{value:"Advantages",id:"advantages",level:2},{value:"Disadvantages",id:"disadvantages",level:2},{value:"Implementation",id:"implementation",level:2},{value:"Kernel Functions",id:"kernel-functions",level:2},{value:"Hyperparameters",id:"hyperparameters",level:2},{value:"Applications",id:"applications",level:2}];function c(e){const n={code:"code",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"svm-regression",children:"SVM Regression"})}),"\n",(0,i.jsxs)(n.p,{children:["Support Vector Machine (SVM) Regression is a powerful ",(0,i.jsx)(n.strong,{children:"supervised learning algorithm"})," that performs regression by finding the optimal hyperplane that maximizes the margin while keeping the error within a specified threshold. Also known as SVR (Support Vector Regression), it's particularly effective for non-linear regression tasks."]}),"\n",(0,i.jsx)(n.h2,{id:"how-svm-regression-works",children:"How SVM Regression Works"}),"\n",(0,i.jsx)(n.p,{children:"Unlike SVM Classification, which tries to find a hyperplane that maximizes the margin between classes, SVM Regression tries to fit a hyperplane to the data that includes as many points as possible within a certain margin (epsilon \u03b5). The key concepts include:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Epsilon-Tube"}),": A tube of width \u03b5 around the hyperplane where errors are not penalized"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Support Vectors"}),": The data points that lie outside the \u03b5-tube"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Kernel Trick"}),": A method to handle non-linear relationships by mapping the data to a higher-dimensional space"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"types-of-svm-regression",children:"Types of SVM Regression"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Epsilon-SVR"}),": Uses an \u03b5-insensitive loss function which ignores errors within \u03b5 distance of the true value"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Nu-SVR"}),": Uses a parameter \u03bd to control the number of support vectors and training errors"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"advantages",children:"Advantages"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Robust to outliers due to the \u03b5-insensitive loss function"}),"\n",(0,i.jsx)(n.li,{children:"Effective in high-dimensional spaces"}),"\n",(0,i.jsx)(n.li,{children:"Versatile due to different kernel functions"}),"\n",(0,i.jsx)(n.li,{children:"Memory efficient (uses only support vectors)"}),"\n",(0,i.jsx)(n.li,{children:"Works well when the number of features is greater than the number of samples"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"disadvantages",children:"Disadvantages"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Computationally intensive for large datasets"}),"\n",(0,i.jsx)(n.li,{children:"Requires careful tuning of hyperparameters"}),"\n",(0,i.jsx)(n.li,{children:"Less interpretable than simpler models"}),"\n",(0,i.jsx)(n.li,{children:"Finding the optimal values for regularization and \u03b5 can be challenging"}),"\n",(0,i.jsx)(n.li,{children:"Not directly suitable for online learning"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"implementation",children:"Implementation"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"from sklearn.svm import SVR\nfrom sklearn.datasets import make_regression\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate a sample regression dataset\nX, y = make_regression(n_samples=200, n_features=1, noise=0.5, random_state=42)\n\n# Add non-linearity to the data\ny = y + np.sin(X[:, 0] * 2) * 5\n\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Scale the features\nscaler_X = StandardScaler()\nX_train_scaled = scaler_X.fit_transform(X_train)\nX_test_scaled = scaler_X.transform(X_test)\n\n# Scale the target variable\nscaler_y = StandardScaler()\ny_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).ravel()\ny_test_scaled = scaler_y.transform(y_test.reshape(-1, 1)).ravel()\n\n# Create and train SVM models with different kernels\nkernels = ['linear', 'poly', 'rbf']\nplt.figure(figsize=(18, 6))\n\nfor i, kernel in enumerate(kernels):\n    # Create and train the model\n    svr_model = SVR(kernel=kernel, gamma='auto', C=10, epsilon=0.1)\n    svr_model.fit(X_train_scaled, y_train_scaled)\n    \n    # Make predictions\n    y_pred_scaled = svr_model.predict(X_test_scaled)\n    y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).ravel()\n    \n    # Evaluate the model\n    mse = mean_squared_error(y_test, y_pred)\n    r2 = r2_score(y_test, y_pred)\n    \n    # Plot results\n    plt.subplot(1, 3, i+1)\n    \n    # Sort points for plot\n    sort_idx = np.argsort(X.ravel())\n    X_sorted = X[sort_idx]\n    y_sorted = y[sort_idx]\n    \n    # Create a mesh grid for predictions across the range\n    X_plot = np.linspace(X.min(), X.max(), 100).reshape(-1, 1)\n    X_plot_scaled = scaler_X.transform(X_plot)\n    y_plot_scaled = svr_model.predict(X_plot_scaled)\n    y_plot = scaler_y.inverse_transform(y_plot_scaled.reshape(-1, 1)).ravel()\n    \n    # Plot the original data points\n    plt.scatter(X, y, color='blue', s=30, alpha=0.5, label='Data points')\n    plt.plot(X_plot, y_plot, color='red', linewidth=2, label=f'SVR with {kernel} kernel')\n    \n    plt.xlabel('Feature')\n    plt.ylabel('Target')\n    plt.title(f'SVR with {kernel} kernel\\nMSE: {mse:.2f}, R\xb2: {r2:.2f}')\n    plt.legend()\n\nplt.tight_layout()\nplt.show()\n\n# Fine-tuning SVR with GridSearchCV\nparam_grid = {\n    'C': [0.1, 1, 10, 100],\n    'gamma': ['scale', 'auto', 0.01, 0.1, 1],\n    'epsilon': [0.01, 0.1, 0.5, 1.0],\n    'kernel': ['rbf']\n}\n\ngrid_search = GridSearchCV(SVR(), param_grid, cv=5, scoring='neg_mean_squared_error', verbose=1)\ngrid_search.fit(X_train_scaled, y_train_scaled)\n\nprint(f\"Best parameters: {grid_search.best_params_}\")\nprint(f\"Best cross-validation score (negative MSE): {grid_search.best_score_:.4f}\")\n\n# Train final model with best parameters\nbest_svr = grid_search.best_estimator_\ny_pred_scaled = best_svr.predict(X_test_scaled)\ny_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).ravel()\n\n# Evaluate final model\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(f\"Final model - MSE: {mse:.4f}, R\xb2: {r2:.4f}\")\n"})}),"\n",(0,i.jsx)(n.h2,{id:"kernel-functions",children:"Kernel Functions"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Linear"}),":\n",(0,i.jsxs)("div",{class:"math",children:["K(x",(0,i.jsx)("sub",{children:"i"}),", x",(0,i.jsx)("sub",{children:"j"}),") = x",(0,i.jsx)("sub",{children:"i"}),(0,i.jsx)("sup",{children:"T"})," x",(0,i.jsx)("sub",{children:"j"})]})]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Suitable for linearly separable data"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Polynomial"}),":\n",(0,i.jsxs)("div",{class:"math",children:["K(x",(0,i.jsx)("sub",{children:"i"}),", x",(0,i.jsx)("sub",{children:"j"}),") = (\u03b3x",(0,i.jsx)("sub",{children:"i"}),(0,i.jsx)("sup",{children:"T"})," x",(0,i.jsx)("sub",{children:"j"})," + r)",(0,i.jsx)("sup",{children:"d"})]})]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Good for non-linear relationships"}),"\n",(0,i.jsx)(n.li,{children:"Parameters: degree d, gamma \u03b3, and coefficient r"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"RBF (Radial Basis Function)"}),":\n",(0,i.jsxs)("div",{class:"math",children:["K(x",(0,i.jsx)("sub",{children:"i"}),", x",(0,i.jsx)("sub",{children:"j"}),") = exp(-\u03b3||x",(0,i.jsx)("sub",{children:"i"})," - x",(0,i.jsx)("sub",{children:"j"}),"||",(0,i.jsx)("sup",{children:"2"}),")"]})]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Most commonly used kernel for non-linear relationships"}),"\n",(0,i.jsx)(n.li,{children:"Works well for most datasets"}),"\n",(0,i.jsx)(n.li,{children:"Parameter: gamma \u03b3 controls the influence of training examples"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Sigmoid"}),":\n",(0,i.jsxs)("div",{class:"math",children:["K(x",(0,i.jsx)("sub",{children:"i"}),", x",(0,i.jsx)("sub",{children:"j"}),") = tanh(\u03b3x",(0,i.jsx)("sub",{children:"i"}),(0,i.jsx)("sup",{children:"T"})," x",(0,i.jsx)("sub",{children:"j"})," + r)"]})]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Related to neural networks"}),"\n",(0,i.jsx)(n.li,{children:"Parameters: gamma \u03b3 and coefficient r"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"hyperparameters",children:"Hyperparameters"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"C"}),": Regularization parameter. Controls the trade-off between model complexity and the degree to which deviations larger than \u03b5 are tolerated."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"epsilon (\u03b5)"}),": Specifies the width of the \u03b5-tube. Points inside the tube do not contribute to the regression fit."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"gamma"}),": Kernel coefficient for 'rbf', 'poly', and 'sigmoid' kernels. Defines how far the influence of a single training example reaches."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"kernel"}),": Type of kernel function to use."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"degree"}),": Degree of the polynomial kernel function."]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"applications",children:"Applications"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Financial market prediction"}),"\n",(0,i.jsx)(n.li,{children:"Time series forecasting"}),"\n",(0,i.jsx)(n.li,{children:"Environmental data analysis"}),"\n",(0,i.jsx)(n.li,{children:"Energy consumption prediction"}),"\n",(0,i.jsx)(n.li,{children:"Chemical process modeling"}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>l,x:()=>a});var r=s(6540);const i={},t=r.createContext(i);function l(e){const n=r.useContext(t);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:l(e.components),r.createElement(t.Provider,{value:n},e.children)}}}]);