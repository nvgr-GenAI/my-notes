"use strict";(self.webpackChunkmy_notes=self.webpackChunkmy_notes||[]).push([[2561],{6387:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>o,contentTitle:()=>l,default:()=>h,frontMatter:()=>t,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"ml/supervised/lasso-regression","title":"Lasso Regression","description":"Lasso (Least Absolute Shrinkage and Selection Operator) Regression is a regularized version of Linear Regression that adds an L1 penalty term to the cost function. This technique performs both variable selection and regularization, effectively shrinking some coefficients exactly to zero.","source":"@site/docs/ml/supervised/lasso-regression.md","sourceDirName":"ml/supervised","slug":"/ml/supervised/lasso-regression","permalink":"/my-notes/docs/ml/supervised/lasso-regression","draft":false,"unlisted":false,"editUrl":"https://github.com/nvgr-GenAI/my-notes/edit/main/docs/ml/supervised/lasso-regression.md","tags":[],"version":"current","frontMatter":{},"sidebar":"mlSidebar","previous":{"title":"Ridge Regression","permalink":"/my-notes/docs/ml/supervised/ridge-regression"},"next":{"title":"Decision Tree Regression","permalink":"/my-notes/docs/ml/supervised/decision-tree-regression"}}');var r=s(4848),a=s(8453);const t={},l="Lasso Regression",o={},d=[{value:"How Lasso Regression Works",id:"how-lasso-regression-works",level:2},{value:"Advantages",id:"advantages",level:2},{value:"Disadvantages",id:"disadvantages",level:2},{value:"Implementation",id:"implementation",level:2},{value:"Selecting the Regularization Parameter (\u03b1)",id:"selecting-the-regularization-parameter-\u03b1",level:2},{value:"Applications",id:"applications",level:2}];function c(e){const n={code:"code",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"lasso-regression",children:"Lasso Regression"})}),"\n",(0,r.jsxs)(n.p,{children:["Lasso (Least Absolute Shrinkage and Selection Operator) Regression is a regularized version of ",(0,r.jsx)(n.strong,{children:"Linear Regression"})," that adds an L1 penalty term to the cost function. This technique performs both variable selection and regularization, effectively shrinking some coefficients exactly to zero."]}),"\n",(0,r.jsx)(n.h2,{id:"how-lasso-regression-works",children:"How Lasso Regression Works"}),"\n",(0,r.jsx)(n.p,{children:"Lasso Regression minimizes the following objective function:"}),"\n",(0,r.jsx)("div",{class:"math",children:(0,r.jsxs)(n.p,{children:["min",(0,r.jsx)("sub",{children:"\u03b2"})," \u03a3",(0,r.jsx)("sub",{children:"i=1"}),(0,r.jsx)("sup",{children:"n"}),"(y",(0,r.jsx)("sub",{children:"i"})," - \u03b2",(0,r.jsx)("sub",{children:"0"})," - \u03a3",(0,r.jsx)("sub",{children:"j=1"}),(0,r.jsx)("sup",{children:"p"}),"\u03b2",(0,r.jsx)("sub",{children:"j"}),"x",(0,r.jsx)("sub",{children:"ij"}),")",(0,r.jsx)("sup",{children:"2"})," + \u03bb\u03a3",(0,r.jsx)("sub",{children:"j=1"}),(0,r.jsx)("sup",{children:"p"}),"|\u03b2",(0,r.jsx)("sub",{children:"j"}),"|"]})}),"\n",(0,r.jsx)(n.p,{children:"Where:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["y",(0,r.jsx)("sub",{children:"i"})," is the target variable"]}),"\n",(0,r.jsxs)(n.li,{children:["x",(0,r.jsx)("sub",{children:"ij"})," are the features"]}),"\n",(0,r.jsxs)(n.li,{children:["\u03b2",(0,r.jsx)("sub",{children:"0"}),", \u03b2",(0,r.jsx)("sub",{children:"j"})," are the coefficients"]}),"\n",(0,r.jsx)(n.li,{children:"\u03bb is the regularization parameter"}),"\n",(0,r.jsxs)(n.li,{children:["The term \u03bb\u03a3",(0,r.jsx)("sub",{children:"j=1"}),(0,r.jsx)("sup",{children:"p"}),"|\u03b2",(0,r.jsx)("sub",{children:"j"}),"| is the L1 regularization penalty"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"advantages",children:"Advantages"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Performs feature selection by shrinking coefficients exactly to zero"}),"\n",(0,r.jsx)(n.li,{children:"Reduces model complexity and prevents overfitting"}),"\n",(0,r.jsx)(n.li,{children:"Creates sparse models with few active coefficients"}),"\n",(0,r.jsx)(n.li,{children:"Improves model interpretability through feature reduction"}),"\n",(0,r.jsx)(n.li,{children:"Effective for high-dimensional datasets"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"disadvantages",children:"Disadvantages"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"May remove relevant features if regularization is too strong"}),"\n",(0,r.jsx)(n.li,{children:"Not stable with highly correlated features (tends to pick one)"}),"\n",(0,r.jsx)(n.li,{children:"Requires careful tuning of the regularization parameter"}),"\n",(0,r.jsx)(n.li,{children:"May underfit with very high regularization"}),"\n",(0,r.jsx)(n.li,{children:"Performance depends heavily on feature scaling"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"implementation",children:"Implementation"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from sklearn.linear_model import Lasso\nfrom sklearn.datasets import make_regression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate sample data\nX, y = make_regression(n_samples=200, n_features=50, noise=0.5, random_state=42)\n\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Scale the features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Create and train the Lasso model with different alpha values\nalphas = [0.001, 0.01, 0.1, 0.5, 1, 10]\nplt.figure(figsize=(12, 8))\n\nfor i, alpha in enumerate(alphas):\n    lasso_model = Lasso(alpha=alpha, max_iter=10000)\n    lasso_model.fit(X_train_scaled, y_train)\n    \n    # Make predictions\n    y_pred = lasso_model.predict(X_test_scaled)\n    \n    # Evaluate the model\n    mse = mean_squared_error(y_test, y_pred)\n    r2 = r2_score(y_test, y_pred)\n    \n    # Count non-zero coefficients\n    n_nonzero = np.sum(lasso_model.coef_ != 0)\n    \n    print(f"Alpha: {alpha}, Non-zero coefficients: {n_nonzero}, MSE: {mse:.4f}, R\xb2: {r2:.4f}")\n    \n    # Plot coefficients\n    plt.subplot(2, 3, i+1)\n    plt.stem(range(X_train.shape[1]), lasso_model.coef_)\n    plt.xlabel(\'Feature\')\n    plt.ylabel(\'Coefficient Value\')\n    plt.title(f\'Alpha = {alpha}\\nNon-zero coef: {n_nonzero}/{X_train.shape[1]}\')\n\nplt.tight_layout()\nplt.show()\n\n# Cross-validation to find optimal alpha\nfrom sklearn.linear_model import LassoCV\nfrom sklearn.model_selection import KFold\n\n# Define range of alphas and cross-validation strategy\nalphas = np.logspace(-4, 1, 50)\ncv = KFold(n_splits=5, shuffle=True, random_state=42)\n\n# Create and fit LassoCV model\nlasso_cv = LassoCV(alphas=alphas, cv=cv, random_state=42, max_iter=10000)\nlasso_cv.fit(X_train_scaled, y_train)\n\n# Print optimal alpha\nprint(f"Optimal Alpha: {lasso_cv.alpha_}")\n\n# Train final model with optimal alpha\noptimal_lasso = Lasso(alpha=lasso_cv.alpha_, max_iter=10000)\noptimal_lasso.fit(X_train_scaled, y_train)\ny_pred = optimal_lasso.predict(X_test_scaled)\n\n# Evaluate final model\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\nn_nonzero = np.sum(optimal_lasso.coef_ != 0)\n\nprint(f"Final Model - Non-zero coefficients: {n_nonzero}/{X_train.shape[1]}")\nprint(f"Final Model - MSE: {mse:.4f}, R\xb2: {r2:.4f}")\n'})}),"\n",(0,r.jsx)(n.h2,{id:"selecting-the-regularization-parameter-\u03b1",children:"Selecting the Regularization Parameter (\u03b1)"}),"\n",(0,r.jsx)(n.p,{children:"The regularization parameter \u03b1 controls the strength of the regularization:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Small \u03b1"}),": Close to ordinary least squares, most features retained"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Large \u03b1"}),": More regularization, more coefficients shrink to zero"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Common methods to select optimal \u03b1 include:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Cross-validation"}),"\n",(0,r.jsx)(n.li,{children:"Information criteria (AIC, BIC)"}),"\n",(0,r.jsx)(n.li,{children:"Domain knowledge"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"applications",children:"Applications"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Feature selection in high-dimensional datasets"}),"\n",(0,r.jsx)(n.li,{children:"Genomics and proteomics analysis"}),"\n",(0,r.jsx)(n.li,{children:"Medical data analysis with many potential predictors"}),"\n",(0,r.jsx)(n.li,{children:"Financial modeling with many potential factors"}),"\n",(0,r.jsx)(n.li,{children:"Signal processing with sparse representation"}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>t,x:()=>l});var i=s(6540);const r={},a=i.createContext(r);function t(e){const n=i.useContext(a);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:t(e.components),i.createElement(a.Provider,{value:n},e.children)}}}]);