---
title: MCP Core Concepts
sidebar_position: 2
description: Deep dive into the core concepts of Model Context Protocol
---

# Model Context Protocol: Core Concepts

The Model Context Protocol (MCP) provides a standardized interface between applications and Large Language Models (LLMs). This document explores the foundational concepts that make MCP powerful and flexible.

## Message-Based Architecture

MCP is built around a message-based architecture where information flows through standardized message objects:

```python
# Example of basic MCP message structure
messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "What is the capital of France?"},
    {"role": "assistant", "content": "The capital of France is Paris."},
    {"role": "user", "content": "What about Japan?"}
]
```

### Message Roles

The protocol defines specific roles for messages:
- **system**: Instructions that guide model behavior
- **user**: Input from the human or application
- **assistant**: Responses generated by the model
- **function**: Information from tool/function calls
- **tool**: Responses from external tools

Each role has a distinct purpose in the conversation flow and helps the model understand the context.

## Context Management

MCP excels at managing conversation context through:

### Context Windows

Most LLMs have a finite context window (e.g., 4K, 8K, or 16K tokens). MCP helps manage this limitation by:

- Tracking token usage across the conversation
- Implementing context window truncation strategies
- Preserving essential information when pruning older messages

```python
# Example of context window management
from langchain.memory import ConversationTokenBufferMemory
from langchain.chat_models import ChatOpenAI

memory = ConversationTokenBufferMemory(
    llm=ChatOpenAI(), 
    max_token_limit=1000
)

# Automatically manages token count within limit
memory.save_context(
    {"input": "What is the capital of France?"}, 
    {"output": "The capital of France is Paris."}
)
```

### Memory Systems

MCP implementations often include various memory types:

1. **Short-term memory**: The immediate conversation history
2. **Long-term memory**: Persistent storage of important information
3. **Working memory**: Active processing space for complex reasoning

## Function Calling

One of MCP's most powerful features is standardized function calling - the ability for models to request external tool execution:

```python
# Example of function definition in MCP
functions = [
    {
        "name": "get_weather",
        "description": "Get the current weather for a location",
        "parameters": {
            "type": "object",
            "properties": {
                "location": {
                    "type": "string",
                    "description": "City and state or country"
                },
                "unit": {
                    "type": "string",
                    "enum": ["celsius", "fahrenheit"],
                    "description": "Temperature unit"
                }
            },
            "required": ["location"]
        }
    }
]

# Using the function with a model
response = openai.ChatCompletion.create(
    model="gpt-4",
    messages=[
        {"role": "user", "content": "What's the weather like in Boston?"}
    ],
    functions=functions,
    function_call="auto"
)
```

### Function Discovery

MCP supports function discovery mechanisms where models can:
- Identify available functions from a registry
- Understand function parameters and return types
- Select appropriate functions based on user requests

### Function Response Handling

After a function executes, MCP provides structured ways to incorporate results:

```python
# Example of handling function responses
function_call = response.choices[0].message.function_call
if function_call:
    function_name = function_call.name
    function_args = json.loads(function_call.arguments)
    
    # Execute the function
    if function_name == "get_weather":
        weather_data = get_weather(function_args["location"], function_args.get("unit", "celsius"))
        
        # Add the function result to messages
        messages.append({
            "role": "function",
            "name": function_name,
            "content": json.dumps(weather_data)
        })
        
        # Get a new response from the model with the function result
        second_response = openai.ChatCompletion.create(
            model="gpt-4",
            messages=messages
        )
```

## Streaming and Progressive Responses

MCP supports streaming responses for better user experience:

```python
# Example of streaming responses
response_stream = openai.ChatCompletion.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "Write a poem about AI"}],
    stream=True
)

# Process chunks as they arrive
for chunk in response_stream:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="", flush=True)
```

This allows applications to:
- Display responses as they're generated
- Provide immediate feedback to users
- Implement cancel mechanisms for long responses

## Error Handling

MCP defines standardized error handling patterns:

1. **Semantic Errors**: When the model fails to understand a request
2. **Context Limit Errors**: When the input exceeds the model's context window
3. **Tool Execution Errors**: When external tools fail to execute properly
4. **Content Policy Violations**: When requests or responses violate usage policies

```python
# Example of error handling
try:
    response = openai.ChatCompletion.create(
        model="gpt-4",
        messages=[{"role": "user", "content": user_input}]
    )
except openai.error.InvalidRequestError as e:
    if "maximum context length" in str(e):
        # Handle context window limitation
        truncated_messages = truncate_context(messages)
        response = openai.ChatCompletion.create(
            model="gpt-4",
            messages=truncated_messages
        )
    else:
        # Handle other request errors
        raise
```

## Implementation Approaches

MCP can be implemented in several ways:

### Direct API Integration

Directly using provider APIs (OpenAI, Anthropic, etc.) with MCP message formats.

### Abstraction Libraries

Using libraries that abstract MCP details:
- LangChain
- LlamaIndex
- Semantic Kernel

### Custom Implementations

Building proprietary MCP implementations for specialized needs.

## Next Steps

Now that you understand the core concepts of MCP:

- Explore [Implementation Patterns](./implementation.md) for practical guidance
- Learn about [Tools & Frameworks](./tools-frameworks.md) for MCP development
- Experiment with the code examples to see MCP in action